{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65c27cfb184dfcc1b4a06bc72c8d6308",
     "grade": false,
     "grade_id": "cell-e7c4ba72adf31577",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 1: Keeping Up With Social Information (Part 1)\n",
    "## Â© Cristian Danescu-Niculescu-Mizil 2020\n",
    "## CS/INFO 4300 Language and Information\n",
    "## Due by midnight on Wednesday January 29th\n",
    "\n",
    "This assignment is **individual**.\n",
    "\n",
    "In this assignment we are practicing with post-processing on a conversational dataset taken from the reality TV show \"Keeping Up With The Kardashians\" and gathering some basic statistics about it. \n",
    "\n",
    "In the next assignment (Assignment 2) we will extend these tools to analyze conversational behavior.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "This project aims to help you get comfortable working with the following tools / technologies / concepts:\n",
    "\n",
    "* word tokenization\n",
    "* histogram plotting using `matplotlib`\n",
    "* character analysis via conversational language\n",
    "* familiarize yourself with basic numpy usage\n",
    "\n",
    "**Academic Integrity and Collaboration**\n",
    "\n",
    "Note that these projects should be completed individually. As a result, all University-standard academic integrity guidelines must be followed.\n",
    "\n",
    "**Guidelines**\n",
    "\n",
    "All cells that contain the blocks that read `# YOUR CODE HERE` are editable and are to be completed to ensure you pass the test-cases. Make sure to write your code where indicated.\n",
    "\n",
    "All cells that read `YOUR ANSWER HERE` are free-response cells that are editable and are to be completed.\n",
    "\n",
    "You may use any number of notebook cells to explore the data and test out your functions, although you will only be graded on the solution itself.\n",
    "\n",
    "\n",
    "You are unable to modify the read-only cells.\n",
    "\n",
    "You should also use Markdown cells to explain your code and discuss your results when necessary.\n",
    "Instructions can be found [here](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "All floating point values should be printed with **2 decimal places** precision. You can do so using the built-in round function.\n",
    "\n",
    "**Grading**\n",
    "\n",
    "For code-completion questions you will be graded on passing the public test cases we have included, as well as any hidden test cases that we have supplemented within a given amount of time to ensure that your logic is correct.\n",
    "\n",
    "Your solution to A1 should finish running in **less than 5 minutes**.\n",
    "For your information, it takes less than 1 minute to run our solution for the entire A1.\n",
    "Also make sure to remove any **redundant print statements** to speed things up and prevent generating unnecessary outputs.\n",
    "\n",
    "For free-response questions you will be manually graded on the quality of your answer.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "You are expected to submit this .ipynb as your submission for Assignment 1. \n",
    "\n",
    "In addition please submit an html copy of the notebook (You can create this by clicking File > Download as > HTML (.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d04cf9cd7a226fa0572c61775b8eefa",
     "grade": false,
     "grade_id": "cell-9d381da04cb83dc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### A0 Review\n",
    "\n",
    "Recall the learning objectives of A0:\n",
    "\n",
    "- The Jupyter Notebook environment\n",
    "- Recap of Python syntax and basic data structures\n",
    "- virtualenv environment for package dependencies\n",
    "\n",
    "We used the BeautifulSoup library to extract episode titles, timestamps, and character speech from HTML files, then stored this data in Python Dictionaries. Finally, we conducted preliminary checks of our data to test its usability (i.e. checking for duplicate transcripts and converting nickname \"Rob\" to \"Robert\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98ee9fc77461651736926701482b1c83",
     "grade": false,
     "grade_id": "cell-97b1c5e4f7df612c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import bs4\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f43944ca7e2ba29b18c82a019e743fea",
     "grade": false,
     "grade_id": "cell-7114417ecfb57e62",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Ensure that your kernel is using Python3\n",
    "assert sys.version_info.major == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "035ea67b59a29b27e265ca7f315e693e",
     "grade": false,
     "grade_id": "cell-1ced8dfe7b8cc493",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Preliminary Data Cleansing\n",
    "**Note: The following content is for you to review to understand how we cleaned and prepared the data for the analysis below.**\n",
    "\n",
    "We will be continuing where we left off from Assignment 0. \n",
    "\n",
    "### Removing duplicates\n",
    "If you are to examine the original transcripts you will see that many of them are near-duplicates, but most are not *perfect* duplicates. This is problematic, because we cannot simply remove identical transcripts.  Furthermore, we cannot just throw away documents that have large overlap, because we would be throwing away the valuable data that is *not* overlapping.\n",
    "\n",
    "We therefore have to treat the transcripts as sequences, rather than as entire documents, and just remove subsequences that overlap.\n",
    "\n",
    "We therefore used a standard python `difflib` package to write the `find_overlaps` function:\n",
    "\n",
    "```python\n",
    "import difflib\n",
    "def find_overlaps(transcript_a, transcript_b, threshold=5):\n",
    "    \"\"\"Find and return the indices of overlapping subsequences between the two transcripts.\n",
    "    Only return overlapping sequences that consist of at least `threshold` entries.\"\"\"\n",
    "    \n",
    "    # We consider that two transcripts overlap when the messages\n",
    "    # and the speakers are the same, but not the timestamp.\n",
    "    \n",
    "    # Massage the transcripts to disregard timestamp information.\n",
    "    # note that a tuple is hashable, so is okay to use for difflib's SequenceMatcher class.\n",
    "    msgs_a = [(m['speaker'], m['text']) if m is not None else None\n",
    "              for m in transcript_a]\n",
    "    msgs_b = [(m['speaker'], m['text']) if m is not None else None\n",
    "              for m in transcript_b]\n",
    "    matcher = difflib.SequenceMatcher(None, msgs_a, msgs_b)\n",
    "    return list(filter(lambda tup: tup[2] >= threshold, matcher.get_matching_blocks()))\n",
    "```\n",
    "\n",
    "We now use the function above to remove duplicate subsequences. At each step, assume we have a list of \"good\" transcripts that have already been processed. When considering a new transcript, we first remove all subsequences that overlap with any of the already processed ones. Then, we split up the chunks that are not removed, and consider each of them a new transcript.\n",
    "\n",
    "```python\n",
    "deduped_transcripts = []\n",
    "all_keys = sorted(transcripts.keys())\n",
    "\n",
    "for key in all_keys:\n",
    "    transcript = transcripts[key]\n",
    "    for _, good_transcript in deduped_transcripts:\n",
    "        overlaps = find_overlaps(transcript, good_transcript)\n",
    "        for idx_a, _, size in overlaps:\n",
    "            transcript[idx_a:idx_a + size] = [None] * size\n",
    "    \n",
    "    for is_not_none, group in groupby(transcript, lambda x: x is not None):\n",
    "        if is_not_none:\n",
    "            subtranscript = list(group)\n",
    "            deduped_transcripts.append((key, subtranscript))\n",
    "```\n",
    "\n",
    "The `deduped_transcripts` are what you are now analyzing for the rest of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31b30dc1f1a7ca25f188c55e99150466",
     "grade": false,
     "grade_id": "cell-e31168ecff8580a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## We are loading the pickle file that contains all the deduped transcripts from Assignment 0\n",
    "with open('deduped_transcripts.pickle','rb') as f:\n",
    "    deduped_transcripts = pickle.load(f)\n",
    "## We are also loading a pickle file of the titles file that we determined in the beginning of Assignment 0\n",
    "with open('titles.pickle','rb') as f:\n",
    "    titles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduped_transcripts.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ac340bfe184ae0ee7ffad51b95850a6",
     "grade": false,
     "grade_id": "cell-6fcb040d322bffc2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Language analysis\n",
    "\n",
    "## Identifying the words\n",
    "It's time to get down to the bread-and-butter of language analysis: the words used.  For simplification, **we consider a word to be a sequence of alphabetical characters. Treat all other characters as delimiters and do not return them.**\n",
    "\n",
    "\n",
    "## Question 1 (Code Completion): Tokenization \n",
    "\n",
    "In the cell below: *Write a function to 'tokenize' a string into the constituent words*. \n",
    "\n",
    "You **must** use regex to satisfy the function specification. We recommend you leverage `re.findall`. \n",
    "\n",
    "Hint: Check out this online regex calculator: [here](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aadb34af6a5b37cab0d19c6180de36f3",
     "grade": false,
     "grade_id": "tokenize",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Returns a list of words that make up the text.\n",
    "    \n",
    "    Note: for simplicity, lowercase everything.\n",
    "    Requirement: Use Regex to satisfy this function\n",
    "    \n",
    "    Params: {text: String}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    T = text.lower()\n",
    "    #r1 = re.findall(r\"\\b[^\\d\\W]+\\b\", T)\n",
    "    r1 = re.split(\"\\d+|\\\\s+|[!@#$%^&*\\'\\\"]\", T)\n",
    "    #r1 = re.findall(r\"\\D+\", T)\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Returns a list of words that make up the text.\n",
    "    \n",
    "    Note: for simplicity, lowercase everything.\n",
    "    Requirement: Use Regex to satisfy this function\n",
    "    \n",
    "    Params: {text: String}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    T = text.lower()\n",
    "    r1 = re.findall(r\"[a-zA-Z]+\", T)\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buy', 'a', 'm', 'face', 's', 'mask']"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring = \"buy a 3m face's mask\"\n",
    "tokenize(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfa570570de81a2a68e0681a984171b3",
     "grade": true,
     "grade_id": "tokenize_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that tokenize returns the correct output\"\"\"\n",
    "assert tokenize(\"It's time 2 get down to the bread-and-butter\") == \\\n",
    "    ['it', 's', 'time', 'get', 'down', 'to', 'the', 'bread', 'and', 'butter']\n",
    "assert tokenize(\"Life, Liberty, & the Pursuit of Happiness\") == \\\n",
    "    ['life', 'liberty', 'the', 'pursuit', 'of', 'happiness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77916091c2a8e9153253797fce5f74cd",
     "grade": false,
     "grade_id": "cell-d753d9a79894c197",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2 (Code Completion): Tokenization of Entire Transcript\n",
    "\n",
    "In the cell below complete the function: *Leveraging the tokenize method to tokenize an entire transcript*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd5360fa5a5a1bed53042ba968768748",
     "grade": false,
     "grade_id": "tokenize_transcript",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_transcript(tokenize_method,input_transcript):\n",
    "    \"\"\"Returns a list of words contained in an entire transcript.\n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             input_transcript: Tuple}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return sum([tokenize_method(text['text']) for text in input_transcript[1]], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1afeb941653d32d8924ddfab99005706",
     "grade": false,
     "grade_id": "cell-c3d95463c66bd217",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deduped_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92be9453913c58275449924259617f4c",
     "grade": true,
     "grade_id": "tokenize_transcript_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that tokenize returns the correct output\"\"\"\n",
    "assert len(tokenize_transcript(tokenize,deduped_transcripts[0])) > 6000 and \\\n",
    "    len(tokenize_transcript(tokenize,deduped_transcripts[0])) < 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae069ddad9be31a4dd862d046cd4063f",
     "grade": false,
     "grade_id": "cell-659f2ecbe807eaef",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3 (Code Completion) Number of Tokens\n",
    "In the cell below write a function to *count how many tokens are used in the deduplicated transcripts in total*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a2c14c70d3852eca3c7d1372f8baa6c",
     "grade": false,
     "grade_id": "num_dedup_tokens",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def num_dedup_tokens(tokenize_method,tokenize_transcript_method,input_transcripts):\n",
    "    \"\"\"Returns number of tokens used in an entire transcript\n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             // Note: Below type means a function that takes two arguments, the first of which is a function.\n",
    "             tokenize_transcript_method: Function ((Function(a -> b), c) -> d),\n",
    "             input_transcripts: Tuple List}\n",
    "    Returns: Integer\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tran_toke = []\n",
    "    for epo in input_transcripts:\n",
    "        toke = tokenize_transcript_method(tokenize_method,epo)\n",
    "        tran_toke = tran_toke + toke\n",
    "    return len(tran_toke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f0a643cce038f516e70acd0971486d1",
     "grade": true,
     "grade_id": "num_dedup_tokens_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that num_dedup_tokens returns the correct output\"\"\"\n",
    "assert num_dedup_tokens(tokenize,tokenize_transcript,deduped_transcripts) > 200000 and \\\n",
    "    num_dedup_tokens(tokenize,tokenize_transcript,deduped_transcripts) < 300000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21444bcf0ce4e6b02466592e96f9b226",
     "grade": false,
     "grade_id": "cell-ed6fa2191c79a632",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4 (Code Completion) Number of Distinct Words\n",
    "\n",
    "In the cell below write a function to *count how many distinct words are in the deduplicated transcripts in total*. \n",
    "\n",
    "Hint: Use a *set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06e992a3ff3cef3cbc736684c9aad4e6",
     "grade": false,
     "grade_id": "num_distinct_words",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def num_distinct_words(tokenize_method,tokenize_transcript_method,input_transcripts):\n",
    "    \"\"\"Returns number of distinct tokens used in an entire transcript\n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             // Note: Below type means a function that takes two arguments, the first of which is a function.\n",
    "             tokenize_transcript_method: Function ((Function(a -> b), c) -> d),\n",
    "             input_transcripts: Tuple List}\n",
    "    Returns: Integer\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tran_toke = []\n",
    "    for epo in input_transcripts:\n",
    "        toke = tokenize_transcript_method(tokenize_method,epo)\n",
    "        tran_toke = tran_toke + toke\n",
    "    tran_toke = set(tran_toke)\n",
    "    return len(tran_toke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf42c86a2de1a7a3356c85a46898ef54",
     "grade": true,
     "grade_id": "num_distict_words_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that num_dedup_tokens returns the correct output\"\"\"\n",
    "assert num_distinct_words(tokenize,tokenize_transcript,deduped_transcripts) > 8000 and \\\n",
    "    num_distinct_words(tokenize,tokenize_transcript,deduped_transcripts) < 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00987786fc42b1e163a69cec6dc37925",
     "grade": false,
     "grade_id": "cell-3d5da2bb92583201",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Question 5 (Code Completion) Word Episode Counts\n",
    "\n",
    "This question is asking you to build a dictionary `word_episode_count[word]` = *number of episodes in which the word appears in*. \n",
    "\n",
    "*Note: Keep in mind that the de-duplicated transcripts don't have unique titles!\n",
    "Recall from A0 that **one episode corresponds to exactly one title** (but not necessarily to only one transcript).*\n",
    "\n",
    "**Your code should ideally take less than 1 second (or a few seconds) to run. If it does not, then you should be able to find a better answer.**\n",
    "\n",
    "In the cell below write a function that: *for each distinct (unique) word, in how many different episodes does it appear?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1feda7a4e78c537c58b61914896db139",
     "grade": false,
     "grade_id": "build_word_episode_count",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_word_episode_count(tokenize_method,tokenize_transcript_method,input_transcripts,input_titles):\n",
    "    \"\"\"Returns a dictionary with the number of episodes each distinct word appears\n",
    "        Params: {tokenize_method: Function (a -> b),\n",
    "                 // Note: Below type means a function that takes two arguments, the first of which is a function.\n",
    "                 tokenize_transcript_method: Function ((Function(a -> b), c) -> d),\n",
    "                 input_transcripts: Tuple List,\n",
    "                 input_titles: Dictionary}\n",
    "        Returns: Dict\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    title_to_token = defaultdict(list)\n",
    "    \n",
    "    # Merge token by title\n",
    "    for transcript in input_transcripts:\n",
    "        title_to_token[input_titles[transcript[0]]] +=  tokenize_transcript_method(tokenize_method, transcript)\n",
    "    all_words = []\n",
    "    for key, value in title_to_token.items():\n",
    "        title_to_token[key] = set(value)\n",
    "        all_words = all_words + list(title_to_token[key])\n",
    "    \n",
    "    return Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a28826d87ccf56c9866f9974edcc8ef8",
     "grade": false,
     "grade_id": "cell-0a14a82efd4f0e23",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "word_episode_count = build_word_episode_count(tokenize,tokenize_transcript,deduped_transcripts,titles)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d8d868508581c5a61fda120a6465ae9",
     "grade": true,
     "grade_id": "build_word_episode_count_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that build_word_episode_count returns the correct output\"\"\"\n",
    "assert word_episode_count['quarter'] == 2\n",
    "assert word_episode_count['made'] == 40\n",
    "assert word_episode_count['never'] == 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f39a4e210aad7edab58517209b80e7d9",
     "grade": false,
     "grade_id": "cell-88307b2867a85c7d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 6 (Free Response): Distribution Analysis\n",
    "\n",
    "For this question you will be plotting a histogram of the distribution of the number of episodes in which words appear. \n",
    "\n",
    "The *x axis* should correspond to the *number of episodes* in which a word is mentioned, and the *y axis* should show *how many words* are in each bin. \n",
    "\n",
    "Note: Use the default matplotlib settings. You may find this tutorial helpful: https://matplotlib.org/3.0.2/tutorials/introductory/pyplot.html\n",
    "\n",
    "Create a new cell with the histogram below. And give an analysis of the histogram: What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83d43ececa794b83dfd9df9ca0bb125b",
     "grade": true,
     "grade_id": "build_word_episode_count_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "word_episode_count.keys()\n",
    "plt.hist(word_episode_count.values(), color='g')\n",
    "plt.ylabel('words count')\n",
    "plt.xlabel('number of episodes')\n",
    "plt.title('distribution of the number of episodes in which words appear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed0e7067a3432750d58976a044e71f6b",
     "grade": false,
     "grade_id": "cell-b78c4d64909ac049",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 7 (Code Completition): Good Types\n",
    "\n",
    "For the following question you will build an alphabetically sorted list of all words that appear in **more than one episode.** We shall refer to these words as *good types*. The strategy here is to eliminate very specific words that occur too rarely. The word *type* here refers to word types. Word types are unique words, i.e. *hello* and *goodbye* are two distinct word types. [\"You say goodbye, and I say hello, hello, hello\"](https://www.youtube.com/watch?v=rblYSKz_VnI&feature=youtu.be&t=16s) (warning -- link will play sound) is a sequence of 9 tokens (ignoring punctuation) and contains one *goodbye* type token, and three *hello* type tokens.\n",
    "\n",
    "In the cell below fulfill the specifications above and have the function: *produce an alphabetically sorted list of good types*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16c604028987a0ca1e0e58e7cebada9f",
     "grade": false,
     "grade_id": "output_good_types",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def output_good_types(input_word_counts):\n",
    "    \"\"\"Returns a list of good types in alphabetically sorted order\n",
    "        Params: {input_word_counts: Dict}\n",
    "        Returns: List\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    good_words = []\n",
    "    for key, value in input_word_counts.items():\n",
    "        if value >1:\n",
    "            good_words.append(key)\n",
    "    good_words.sort() \n",
    "    return good_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dbf338f4231a604985d91122c842938",
     "grade": false,
     "grade_id": "cell-8f0bb8ce404b6b24",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "good_types = output_good_types(word_episode_count)\n",
    "n_good_types = len(good_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05c822ec869f48bd52d36605cf2234a0",
     "grade": true,
     "grade_id": "output_good_types_test",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that output_good_types returns the correct output\"\"\"\n",
    "assert n_good_types > 4500 and n_good_types < 5000\n",
    "assert good_types[0:5] == ['a','aah','ability','able','about']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2dc7e54479c1322047653786c35ecf7",
     "grade": false,
     "grade_id": "cell-800cf3466c807a90",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 7b (Free Response): Good Types\n",
    "\n",
    "In the cell below answer the following: *How many good_types there are? What are the first 10 in alphabetical order?*\n",
    "\n",
    "Please write your answer in code and use Python's `print()` function, **NOT** markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92ab05da789cb6c60699f2e1842da6c2",
     "grade": true,
     "grade_id": "output_good_types_ans",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4517\n",
      "['a', 'aah', 'ability', 'able', 'about', 'absolute', 'absolutely', 'absurd', 'accent', 'accept']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(len(good_types))\n",
    "print(good_types[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75e87c5da88423813461167eabd48dff",
     "grade": false,
     "grade_id": "cell-dd25b6c0ee0f7aa7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "From now on, the use of array data structures from `numpy` is required in some places. If you've never used numpy before, it will take some time to get used to. In general, numpy is a library that you can use to handle vector/matrix/tensor operations, including creation, modification, and compositions (add, sub, mul, etc.). For example, try running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d70575ee2520d8663a5626a206cd5f6",
     "grade": false,
     "grade_id": "cell-a35c29d2a336c43f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 6 2]]\n",
      "[[1 2]\n",
      " [5 3]\n",
      " [1 1]]\n",
      "[[14 11]\n",
      " [36 28]]\n",
      "[5 8 5]\n",
      "[3 8 2]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3], [4,6,2]]) \n",
    "B = np.array([[1,2],[5,3],[1,1]])\n",
    "print(A) #A is a 2x3 matrix\n",
    "print(B) #B is a 3x2 matrix\n",
    "print(A.dot(B)) #A.dot(B) is a 2 by 2 matrix\n",
    "print(np.sum(A, axis=0)) #np.sum(A, axis=0) sums along columns\n",
    "print(np.sum(B, axis=1)) #np.sum(B, axis=1) sums along rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "609638d96f460c027d4bd14392cfbda2",
     "grade": false,
     "grade_id": "cell-be083b1847639a79",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You're welcome to find your own resources to learn more about numpy (there are lots of them) but one good introduction is [Justin Johnson's writeup](http://cs231n.github.io/python-numpy-tutorial/#numpy).\n",
    "\n",
    "We will be using vectors and arrays with *n_good_types* columns, such that each good type corresponds to a column, in alphabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bae2c2e7b691e6aa5a954170cf2cc807",
     "grade": false,
     "grade_id": "cell-f473f9073805a3f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 8 (Code Completion): Word Frequencies\n",
    "\n",
    "What can we say about the most frequently used words? Briefly consider, would you expect to find the same ordering of frequent words in, say, the NY Times?\n",
    "\n",
    "Note that we are talking about the frequency a word appears across the entire corpus, rather than how many episodes a word appears in.\n",
    "\n",
    "In the cell below, complete the function to *find the word frequency of all \"good types\"* in descending order.\n",
    "\n",
    "Notes: corpus is a collection of written texts, in this case the transcripts of a reality TV show. Frequency is the rate at which something occurs.\n",
    "\n",
    "For this question only, **round your solution to 5 decimals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "529784228b0363a12e7350835db1cdff",
     "grade": false,
     "grade_id": "create_ranked_good_types",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def create_ranked_good_types(tokenize_method,tokenize_transcript_method,input_transcripts,input_good_types):\n",
    "    \"\"\"Returns a list of good types in reverse sorted order in the form:\n",
    "        [(word_1,word_frequency_1),\n",
    "        ...\n",
    "        (word_10,word_frequency_10)]\n",
    "        Params: {tokenize_method: Function (a -> b),\n",
    "                 // Note: Below type means a function that takes two arguments, the first of which is a function.\n",
    "                 tokenize_transcript_method: Function ((Function(a -> b), c) -> d),\n",
    "                 input_transcripts: Tuple List,\n",
    "                 input_good_types: List}\n",
    "        Returns: List\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    all_words=[]\n",
    "    for i in range(len(input_transcripts)):\n",
    "        all_words =  all_words + tokenize_transcript_method(tokenize_method,input_transcripts[i])\n",
    "    all_dict = Counter(all_words)\n",
    "    length = len(all_words)\n",
    "    \n",
    "    good_words_dict = {}\n",
    "    for word in input_good_types:\n",
    "        good_words_dict[word] = round(all_dict[word]/length,5)\n",
    "           \n",
    "    listofTuples = sorted(good_words_dict.items() ,  key=lambda x: x[1] , reverse = True)\n",
    "\n",
    "    return listofTuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a10990e2d310b85158ef32548e69d21",
     "grade": false,
     "grade_id": "cell-8bb7a64aaa1b1620",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ranked_frequencies = create_ranked_good_types(tokenize,tokenize_transcript,deduped_transcripts,good_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "361db5f3f27911fcd994ded2e6a6f93f",
     "grade": false,
     "grade_id": "cell-098c443e5f1380a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Suggestion: Sum the word frequencies found for all good types. Consider why they do not add to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798599999999208"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for (a, b) in ranked_frequencies:\n",
    "    total +=b\n",
    "total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b070b5f9995d24d01f462b6aa40d0faf",
     "grade": true,
     "grade_id": "create_ranked_good_types_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_ranked_good_types returns the correct output\"\"\"\n",
    "assert ranked_frequencies[0:2] == [('i', 0.04858), ('you', 0.03797)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 0.04858), ('you', 0.03797)]"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_frequencies[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2897f70bebef640bcba17420e965a444",
     "grade": false,
     "grade_id": "cell-3f52d6dd8641a84e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Characterizing characters' language\n",
    "\n",
    "Moving on, we will only be considering a subset of characters, arguably the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddbabcc279eb2d98193eb9181daf839d",
     "grade": false,
     "grade_id": "cell-87f62951498c6a21",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "good_speakers = [u'BRUCE',\n",
    "                 u'JONATHAN',\n",
    "                 u'KHLOE',\n",
    "                 u'KIM',\n",
    "                 u'KOURTNEY',\n",
    "                 u'KRIS',\n",
    "                 u'ROBERT',\n",
    "                 u'SCOTT']\n",
    "\n",
    "n_speakers = len(good_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KHLOE'"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_speakers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9120b489de3ed3ed3ef21bb46d45d067",
     "grade": false,
     "grade_id": "cell-1e73cf04c0942e6e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 9 (Code Completion): Character Word Occurrences\n",
    "\n",
    "In the cell below you will be asked to determine the *occurrences words by each character*\n",
    "\n",
    "This function **requires** you to return a numpy array of shape `n_speakers` by `n_good_types` such that the entry `(i,j)` indicates how many times speaker i says word j. \n",
    "\n",
    "Note: you will lose points if you do not use numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08b1421e38b2e3cf421b8b2b29ffae63",
     "grade": false,
     "grade_id": "create_word_freq_array",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_word_occurrence_matrix(\n",
    "    tokenize_method,\n",
    "    input_transcripts,\n",
    "    input_speakers,\n",
    "    input_good_types):\n",
    "    \"\"\"Returns a numpy array of shape n_speakers by n_good_types such that the \n",
    "    entry (ij) indicates how often speaker i says word j.\n",
    "    \n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             input_transcripts: Tuple List,\n",
    "             input_speakers: List,\n",
    "             input_good_types: List}\n",
    "    Returns: Numpy Array\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    A = np.zeros((len(input_speakers), len(input_good_types)))\n",
    "    \n",
    "    speaker_word = defaultdict(list)\n",
    "    for transcript in input_transcripts:\n",
    "        for dic in transcript[1]:\n",
    "            speaker_word[dic['speaker']] += tokenize_method(dic['text'])\n",
    "    \n",
    "    \n",
    "    for i in range(len(input_speakers)):\n",
    "        for j in range(len(input_good_types)):\n",
    "            A[i,j] = speaker_word[input_speakers[i]].count(input_good_types[j])\n",
    "            \n",
    "    return A\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    A = np.zeros((len(input_speakers), len(input_good_types)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(input_speakers)):\n",
    "        words = []\n",
    "        for transcript in input_transcripts:\n",
    "            for dic in transcript[1]:\n",
    "                if dic['speaker'] == input_speakers[i]:\n",
    "                    words = words + tokenize_method(dic['text'])\n",
    "                    \n",
    "        for j in range(len(input_good_types)):\n",
    "            A[i,j] = words.count(input_good_types[j])\n",
    "            \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de35d2ee91f5379e42914f23d401db0d",
     "grade": false,
     "grade_id": "cell-4229598e63922f3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "word_matrix = create_word_occurrence_matrix(tokenize,deduped_transcripts,good_speakers,good_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bd6209da6addb75fc735b6b66230221",
     "grade": true,
     "grade_id": "create_word_freq_array_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_word_freq_array returns the correct output\"\"\"\n",
    "assert sum(word_matrix[0]) > 26000.0\n",
    "assert sum(word_matrix[:,3]) > 40 and sum(word_matrix[:,3]) < 50\n",
    "assert type(word_matrix) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e63a83b716c540c95425968e7f471511",
     "grade": false,
     "grade_id": "cell-55cdd926451a2321",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 9b (Free Response): Character Word Occurrences\n",
    "\n",
    "In the cell below, output the *top 10 most occurring words used by each character* in the following format (if you encounter ties, then any ordering is acceptable):\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "CHARACTER_NAME_A\n",
    "word_1\n",
    "word_2\n",
    "...\n",
    "word_10\n",
    "\n",
    "CHARACTER_NAME_B\n",
    "word_1\n",
    "word_2\n",
    "...\n",
    "word_10\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78044ad7e01b7c42169efbf0ef37df2b",
     "grade": true,
     "grade_id": "create_word_freq_array_ans",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUCE\n",
      "i\n",
      "you\n",
      "the\n",
      "s\n",
      "to\n",
      "it\n",
      "a\n",
      "and\n",
      "that\n",
      "this\n",
      "\n",
      "JONATHAN\n",
      "i\n",
      "you\n",
      "to\n",
      "the\n",
      "s\n",
      "a\n",
      "it\n",
      "and\n",
      "like\n",
      "this\n",
      "\n",
      "KHLOE\n",
      "i\n",
      "you\n",
      "to\n",
      "and\n",
      "s\n",
      "a\n",
      "the\n",
      "it\n",
      "that\n",
      "like\n",
      "\n",
      "KIM\n",
      "i\n",
      "you\n",
      "to\n",
      "and\n",
      "s\n",
      "the\n",
      "it\n",
      "a\n",
      "that\n",
      "like\n",
      "\n",
      "KOURTNEY\n",
      "i\n",
      "to\n",
      "you\n",
      "and\n",
      "s\n",
      "it\n",
      "the\n",
      "a\n",
      "that\n",
      "t\n",
      "\n",
      "KRIS\n",
      "i\n",
      "you\n",
      "to\n",
      "s\n",
      "the\n",
      "a\n",
      "and\n",
      "it\n",
      "that\n",
      "t\n",
      "\n",
      "ROBERT\n",
      "i\n",
      "to\n",
      "you\n",
      "s\n",
      "and\n",
      "a\n",
      "the\n",
      "just\n",
      "it\n",
      "that\n",
      "\n",
      "SCOTT\n",
      "i\n",
      "you\n",
      "to\n",
      "s\n",
      "the\n",
      "a\n",
      "it\n",
      "that\n",
      "and\n",
      "t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "word_matrix.shape\n",
    "for i in range(word_matrix.shape[0]):\n",
    "    word_sort_index = list(np.argsort(word_matrix[i]))\n",
    "    print(good_speakers[i])\n",
    "    find_index = word_sort_index[::-1][:10]\n",
    "    for j in find_index:\n",
    "        print(good_types[j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d53fe3b339d5c1e7e54b4f962605b432",
     "grade": false,
     "grade_id": "cell-fe5bd652435d4c18",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 9c (Code Completion): Character Word Occurrences\n",
    "\n",
    "Using your `word_matrix`, identify the characters (in descending order of number of occurrences) who have said **all** of the following words: `[\"shopping\", \"sorry\", \"sister\"]`. \n",
    "\n",
    "Please note: a character must have spoken each word at least once to be included in your answer (if you encounter ties, then any ordering is acceptable).\n",
    "\n",
    "In the cell below, output your answer in the following format:\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "shopping\n",
    "CHARACTER_NAME_A occurrences_shopping_A\n",
    "CHARACTER_NAME_B occurrences_shopping_B\n",
    "...\n",
    "\n",
    "sorry\n",
    "CHARACTER_NAME_A occurrences_sorry_A\n",
    "CHARACTER_NAME_B occurrences_sorry_B\n",
    "...\n",
    "\n",
    "sister\n",
    "CHARACTER_NAME_A occurrences_sister_A\n",
    "CHARACTER_NAME_B occurrences_sister_B\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c1a474773827438dd6aea7488f3f1f2",
     "grade": false,
     "grade_id": "cell-a1c70f3e0648642d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "interesting_words = [\"shopping\", \"sorry\", \"sister\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4717101a207f631b43797cd2f7e5e0f4",
     "grade": true,
     "grade_id": "cell-51c7d3273a979ae8",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shopping\n",
      "KIM 41.0\n",
      "KOURTNEY 37.0\n",
      "KRIS 6.0\n",
      "KHLOE 6.0\n",
      "BRUCE 2.0\n",
      "\n",
      "sorry\n",
      "KHLOE 38.0\n",
      "KRIS 34.0\n",
      "KIM 31.0\n",
      "ROBERT 13.0\n",
      "KOURTNEY 12.0\n",
      "JONATHAN 9.0\n",
      "SCOTT 7.0\n",
      "BRUCE 4.0\n",
      "\n",
      "sister\n",
      "KOURTNEY 25.0\n",
      "KHLOE 22.0\n",
      "KRIS 13.0\n",
      "KIM 9.0\n",
      "SCOTT 6.0\n",
      "ROBERT 6.0\n",
      "BRUCE 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "for word in interesting_words:\n",
    "    index_interesting = good_types.index(word)\n",
    "    \n",
    "    word_matrix[:, index_interesting]\n",
    "    frequency_sort_index = list(np.argsort(word_matrix[:, index_interesting]))\n",
    "    frequency_sort_index\n",
    "    frequency_sort_index_new = []\n",
    "    for index in frequency_sort_index:\n",
    "        if word_matrix[index, index_interesting] > 0:\n",
    "            frequency_sort_index_new.append(index)\n",
    "    frequency_sort_index_new\n",
    "    frequency_sort_index_reverse = frequency_sort_index_new[::-1]\n",
    "    print(word)\n",
    "    for index in frequency_sort_index_reverse:\n",
    "        print(good_speakers[index]+' ' + str(word_matrix[index, index_interesting]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1d3d03b6e0e02109a57286f67fa7a3b",
     "grade": false,
     "grade_id": "cell-6a3402cec2421178",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 10 (Code Completion): Word Character Counts\n",
    "\n",
    "In the questions below you will now be asked to determine *how many characters have used a specific word*? For example, how many characters have ever said \"botox\" in the show?\n",
    "\n",
    "The function below will require you to use the `word_matrix` from above to return a 1-D numpy array that reports the number of `good_speakers` that have uttered each word within `good_types`.\n",
    "The i-th entry of your answer array `word_character_count_array[i]` should be the number of characters that have uttered the word `good_types[i]`.\n",
    "\n",
    "Hint! Numpy is your friend :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1f9f25cdc758dd584f1a858f0ba640e",
     "grade": false,
     "grade_id": "create_word_character_count_array",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_word_character_count_array(word_freq_matrix):\n",
    "    \"\"\"Returns a numpy array of shape (n_good_types,) such that the \n",
    "    entry i indicates how many good_speakers have uttered word i.\n",
    "    \n",
    "    Params: { word_freq_matrix: a numpy matrix of shape (n_speakers, n_good_types) }\n",
    "    \n",
    "    Hint: You may want to consult the numpy documentation to make this easy.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    count_list = []\n",
    "    for i in range(len(good_types)):\n",
    "        word_freq = list(word_freq_matrix[:, i])\n",
    "        count = 0\n",
    "        for i in word_freq:\n",
    "            if i >0:\n",
    "                count+=1\n",
    "            \n",
    "        count_list.append(count)\n",
    "    return np.array(count_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "738b9e08086a81cc2f3bc0458cd2ec63",
     "grade": false,
     "grade_id": "cell-11c569836da724f2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "word_character_count_array = create_word_character_count_array(word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39eb8256427b7b144074e89f22a87895",
     "grade": true,
     "grade_id": "create_word_character_count_array_tests",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_character_word_count_array returns the correct output\"\"\"\n",
    "assert type(word_character_count_array) == np.ndarray\n",
    "assert word_character_count_array[0] == 8\n",
    "assert word_character_count_array[1] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8330b156e500095045c6a7bf0b02bc7c",
     "grade": false,
     "grade_id": "cell-bb8e24a9dd852d39",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 10b (Free Response): Word Character Counts\n",
    "\n",
    "What are the first 10 words (sorted in alphabetical order) that are said by **at least 5** characters?\n",
    "\n",
    "In the cell below, give your answer in the following format:\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "word_1\n",
    "word_2\n",
    "...\n",
    "word_10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5018e92208134fad276a412c1c6a1d3",
     "grade": true,
     "grade_id": "cell-f64719c7d57ec17e",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'able', 'about', 'absolutely', 'accept']"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "word_5_chara = []\n",
    "for index in range(len(word_character_count_array)):\n",
    "    if word_character_count_array[index]>=5:\n",
    "        word_5_chara.append(good_types[index])\n",
    "word_5_chara = sorted(word_5_chara, reverse = False)\n",
    "word_5_chara[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec8f9728596885f1e95916d484a05e7c",
     "grade": false,
     "grade_id": "cell-e055c738cea3d3b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 11 (Code Completion): Specific Word Usage by Character\n",
    "\n",
    "The exercise in 9b didn't help much in understanding each character's diction, because common words are used too commonly anyway. We want to give more weight to less frequent words, as they carry more information on the particularities of the characters.\n",
    "\n",
    "A simple way to do this is to score the words according to the ratio between how often a given character said the word and how often any of the *good speakers* said it.  \n",
    "\n",
    "This can be accomplished by dividing each columns in the `word_array` matrix by its sum.\n",
    "\n",
    "**Note: as some words might never be said by the key characters we are considering, add 1 to the sum of each column to avoid division by 0**\n",
    "\n",
    "In the cell below, complete the function to return a weighted numpy array of *specific* words used by each character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1ea90cbacacbac32b6d87c3d0af67d2",
     "grade": false,
     "grade_id": "create_weighted_word_freq_array",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def create_weighted_word_freq_array(input_word_array):\n",
    "    \"\"\"Returns a numpy array of shape n_speakers by n_good_types such that the \n",
    "    entry (ij) indicates how often speaker i says word j weighted by the above ratio.\n",
    "    \n",
    "    Note: You must add 1 to the sum of each column to avoid divison by 0 issues.\n",
    "    \n",
    "    Params: {input_word_array: Numpy Array}\n",
    "    Returns: Numpy Array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    word_array = copy.deepcopy(word_matrix)\n",
    "    column_sum = []\n",
    "    for i in range(input_word_array.shape[1]):\n",
    "        column_sum.append(np.sum(input_word_array[:, i]))\n",
    "    for j in range(input_word_array.shape[1]):\n",
    "        word_array[:, j] = input_word_array[:, j]/(column_sum[j]+1)\n",
    "    return word_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2645c98bf45be9203653a54c95954b38",
     "grade": false,
     "grade_id": "cell-f3e0cd3d8309ecea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "weighted_words = create_weighted_word_freq_array(word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4a443b75be216be377f68a9efd0e8ea",
     "grade": true,
     "grade_id": "create_weighted_word_freq_array_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_word_freq_array returns the correct output\"\"\"\n",
    "assert sum(weighted_words[:,7]) > 0.7\n",
    "assert type(weighted_words) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b413bd3c0044888cb3facb38075b8e6",
     "grade": false,
     "grade_id": "cell-b2b74adae027c815",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 11b (Free Response): Specific Word Usage by Character\n",
    "\n",
    "Use the next cell to output the top 10 most *specific* words (in descending order) used by each character in the following format:\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "CHARACTER_NAME_A\n",
    "score_1 word_1\n",
    "score_2 word_2\n",
    "...\n",
    "score_10 word_10\n",
    "\n",
    "CHARACTER_NAME_B\n",
    "score_1 word_1\n",
    "score_2 word_2\n",
    "...\n",
    "score_10 word_10\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a97760c30b734888024f66f68e96ed59",
     "grade": true,
     "grade_id": "create_weighted_word_freq_array_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUCE\n",
      "genetic\n",
      "hobby\n",
      "planners\n",
      "brake\n",
      "carpool\n",
      "presentation\n",
      "robbie\n",
      "mcdonald\n",
      "toughest\n",
      "surgeries\n",
      "\n",
      "JONATHAN\n",
      "erika\n",
      "pics\n",
      "katie\n",
      "awareness\n",
      "simon\n",
      "command\n",
      "oats\n",
      "beckham\n",
      "carmen\n",
      "homely\n",
      "\n",
      "KHLOE\n",
      "fur\n",
      "basic\n",
      "apparently\n",
      "secure\n",
      "campaign\n",
      "furs\n",
      "moral\n",
      "begins\n",
      "fulfilled\n",
      "vandalizing\n",
      "\n",
      "KIM\n",
      "amusing\n",
      "song\n",
      "frizz\n",
      "challenge\n",
      "humiliating\n",
      "punched\n",
      "hotwire\n",
      "airport\n",
      "signs\n",
      "advantage\n",
      "\n",
      "KOURTNEY\n",
      "ho\n",
      "defensive\n",
      "escalated\n",
      "busted\n",
      "version\n",
      "theb\n",
      "absurd\n",
      "sober\n",
      "driveway\n",
      "cushion\n",
      "\n",
      "KRIS\n",
      "sweetie\n",
      "cristal\n",
      "cranky\n",
      "shops\n",
      "kenneth\n",
      "los\n",
      "angeles\n",
      "chat\n",
      "maker\n",
      "department\n",
      "\n",
      "ROBERT\n",
      "email\n",
      "anal\n",
      "truly\n",
      "adrian\n",
      "scream\n",
      "acceptable\n",
      "overnight\n",
      "spoken\n",
      "erection\n",
      "cheating\n",
      "\n",
      "SCOTT\n",
      "sitter\n",
      "exclamation\n",
      "gentlemen\n",
      "hooker\n",
      "fella\n",
      "fabu\n",
      "brucinator\n",
      "custom\n",
      "squat\n",
      "thr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "weighted_words.shape\n",
    "for i in range(weighted_words.shape[0]):\n",
    "    word_sort_index = list(np.argsort(weighted_words[i]))\n",
    "    print(good_speakers[i])\n",
    "    find_index = word_sort_index[::-1][:10]\n",
    "    for j in find_index:\n",
    "        print(good_types[j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed40416ff151e90cd4551c782e4c46a7",
     "grade": false,
     "grade_id": "cell-8c3a58ca841fda7e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 11c (Free Response): Specific Word Usage by Character\n",
    "\n",
    "Now we can start to see interesting differences between the characters.\n",
    "\n",
    "Create a new Markdown cell bellow and use it to write a paragraph discussing the differences you find most striking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d2a834211251c3d7af0d6cf774eb661",
     "grade": true,
     "grade_id": "word_usage_analysis",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "Before i calculate weighted word frequency, the top 10 most specific words used by each character are mostly some stopwords or irrelevent words like this, that, the and so on.\n",
    "the result is not useful to tell which word is important for a character.\n",
    "however, after i divide each columns in the word_array matrix by its sum, then some words like this, the denominator is quite large, therefore 'this' is deluted by large denominator.\n",
    "as i choose the top 10 most specific words, 'this' does not appear in the top list. instead, some words which are not frequent among all the speaker appear in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e25b394b4f8fa54b565cc97fc19df8d3",
     "grade": false,
     "grade_id": "cell-ed4cc4acf7ec52c9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "This is the end of Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
