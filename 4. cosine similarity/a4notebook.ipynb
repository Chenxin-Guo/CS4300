{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96637e4d374891f9d1f2b0e00d739aa4",
     "grade": false,
     "grade_id": "cell-05fb407e20c068e6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 4: \"Search your transcripts. You will know it to be true.\" (Part 2)\n",
    "\n",
    "## Â© Cristian Danescu-Niculescu-Mizil 2019\n",
    "\n",
    "## CS/INFO 4300 Language and Information\n",
    "\n",
    "### Due by 11:59pm on Wednesday February 20th\n",
    "\n",
    "\n",
    "This is an **individual** assignment.\n",
    "\n",
    "If you use any outside sources (e.g. research papers, StackOverflow) please list your sources.\n",
    "\n",
    "In our last assignment we have explored edit distance to retrieve similar sounding quotes from the transcripts. Our overall goal in this assignment is to build a system that efficiently searches for documents similar to a query in large data sets. We will explore the tradeoffs of information retrieval systems by finding newspaper quotes from \"Keeping Up With The Kardashians\".\n",
    "\n",
    "**Guidelines**\n",
    "\n",
    "All cells that contain the blocks that read `# YOUR CODE HERE` are editable and are to be completed to ensure you pass the test-cases. Make sure to write your code where indicated.\n",
    "\n",
    "All cells that read `YOUR ANSWER HERE` are free-response cells that are editable and are to be completed.\n",
    "\n",
    "You may use any number of notebook cells to explore the data and test out your functions, although you will only be graded on the solution itself.\n",
    "\n",
    "You are unable to modify the read-only cells and should never delete any of the given cells.\n",
    "\n",
    "You should also use Markdown cells to explain your code and discuss your results when necessary.\n",
    "Instructions can be found [here](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "All floating point values should be printed with **2 decimal places** precision. You can do so using the built-in round function.\n",
    "\n",
    "No cell in this assignment should take longer than **1 second** to run. If a cell takes longer than 1 second to run, it will be marked as incorrect.\n",
    "\n",
    "**Grading**\n",
    "\n",
    "For code-completion questions you will be graded on passing the public test cases we have included, as well as any hidden test cases that we have supplemented to ensure that your logic is correct.\n",
    "\n",
    "For free-response questions you will be manually graded on the quality of your answer.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "- Develop an understanding of the inverted index and its applications\n",
    "- Explore use cases of boolean search\n",
    "- Examine how the inverted index can be used to efficiently compute IDF values\n",
    "- Introduce cosine similarity as an efficient search model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b513f1577dbc3c3fe7da1cc2fb78803b",
     "grade": false,
     "grade_id": "cell-80fc97f06dc7715b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import math\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c2b0f8d6bb1ec27746c1dfd4fa646a3",
     "grade": false,
     "grade_id": "cell-977931635a8ef8aa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851\n"
     ]
    }
   ],
   "source": [
    "with open(\"kardashian-transcripts.json\", \"r\") as f:\n",
    "    transcripts = json.load(f)\n",
    "print(len(transcripts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e11948a015dd145cd3bc1683d2a46108",
     "grade": false,
     "grade_id": "cell-ef81cb8deedd1818",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "flat_msgs = [m for transcript in transcripts for m in transcript]\n",
    "queries = [u\"It's like a bunch of people running around talking about nothing.\",\n",
    "           u\"Never say to a famous person that this possible endorsment would bring 'er to the spot light.\",\n",
    "           u\"Your yapping is making my head ache!\",\n",
    "           u\"I'm going to Maryland, did I tell you?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86e0e502b19c66526fc8c25400310788",
     "grade": false,
     "grade_id": "cell-30f8447c4e8aec09",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Finding the most similar messages (cosine similarity)\n",
    "\n",
    "### A high-level overview\n",
    "\n",
    "Our overall goal of the last part of this assignment is to build a system where we can compute the cosine similarity between queries and our datasets quickly. To accomplish queries and compute cosine similarities, we will need to represent documents as vectors. A common method of representing documents as vectors is by using \"term frequency-inverse document frequency\" (tf-idf) scores. More details about this method can be found [on the course website](http://www.cs.cornell.edu/courses/cs4300/2020sp/Slides//vsm_cheatsheet.pdf). The notation here is consistent with the hand out, so if you haven't read over it -- you should!\n",
    "\n",
    "Consider the tf-idf representation of a document and a query: $\\vec{d_j}$ and $\\vec{q}$, respectively. Elements of these vectors are very often zero because the term frequency of most words in most documents is zero. Stated differently, most words don't appear in most documents! Consider a query that has 5 words in it and a vocabulary that has 20K words in it -- only .025% of the elements of the vector representation of the query are nonzero! When a vector (or a matrix) has very few nonzero entries, it is called \"sparse.\" We can take advantage of the sparsity of tf-idf document representations to compute cosine similarity quickly. We will first build some data stuctures that allow for faster querying of statistics, and then we will build a function that quickly computes cosine similarity between queries and documents.\n",
    "\n",
    "### A starting point\n",
    "We will use an **inverted index** for efficiency. This is a sparse term-centered representation that allows us to quickly find all documents that contain a given term.\n",
    "\n",
    "## Q1 Write a function to construct the inverted index (Code Completion)\n",
    "\n",
    "As in class, the inverted index is a key-value structure where the keys are terms and the values are lists of *postings*. In this case, we record the documents a term occurs in as well as the **count** of that term in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bda30fd8612c4f99c56924db18e956cb",
     "grade": false,
     "grade_id": "build_inverted_index",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_inverted_index(msgs):\n",
    "    \"\"\" Builds an inverted index from the messages.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    msgs: list of dicts.\n",
    "        Each message in this list already has a 'toks'\n",
    "        field that contains the tokenized message.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    inverted_index: dict\n",
    "        For each term, the index contains \n",
    "        a sorted list of tuples (doc_id, count_of_term_in_doc)\n",
    "        such that tuples with smaller doc_ids appear first:\n",
    "        inverted_index[term] = [(d1, tf1), (d2, tf2), ...]\n",
    "        \n",
    "    Example\n",
    "    =======\n",
    "    \n",
    "    >> test_idx = build_inverted_index([\n",
    "    ...    {'toks': ['to', 'be', 'or', 'not', 'to', 'be']},\n",
    "    ...    {'toks': ['do', 'be', 'do', 'be', 'do']}])\n",
    "    \n",
    "    >> test_idx['be']\n",
    "    [(0, 2), (1, 2)]\n",
    "    \n",
    "    >> test_idx['not']\n",
    "    [(0, 1)]\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    d = defaultdict(list)\n",
    "    for k in range(len(msgs)):\n",
    "        for word in set(msgs[k]['toks']):\n",
    "            d[word].append((k, msgs[k]['toks'].count(word)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31126aff0fb57f211e2e1e16981abe86",
     "grade": true,
     "grade_id": "build_inverted_index_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "start_time = time.time()\n",
    "inv_idx = build_inverted_index(flat_msgs)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert len(inv_idx) <= 10000 \n",
    "assert [i[0] for i in inv_idx['bruce']] == sorted([i[0] for i in inv_idx['bruce']])\n",
    "assert len(inv_idx['bruce']) < len(inv_idx['kim'])\n",
    "assert len(inv_idx['bruce']) >= 400 and len(inv_idx['bruce']) <= 435\n",
    "assert len(inv_idx['baby']) >= 250 and len(inv_idx['baby']) <= 300\n",
    "assert execution_time <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff44a2eb5324340bc33f6a1629216b0e",
     "grade": false,
     "grade_id": "cell-c92587934d16cc87",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q2 Using the inverted index for boolean search (Code Completion)\n",
    "\n",
    "In this section we will use the inverted index you constructed to perform an efficient boolean search. The boolean model was one of the early information retrieval models, and continues to be used in applications today.\n",
    "\n",
    "A boolean search works by searching for documents which match the boolean expression of the query. Three main operators in a boolean search are `AND` `OR` and `NOT`. For example, the query `\"Ned\" and \"Rob\"` would return any document which contains both the words \"Ned\" and \"Rob\".\n",
    "\n",
    "Here, we will treat a query as a simple two-word search with exclusion. For example, the query words \"kardashian\", \"kim\" would be equivalent to the boolean expression `\"kardashian\" NOT \"kim\"`.\n",
    "\n",
    "#### In class we implemented the Merge Postings Algorithm, review the code [here](https://www.cs.cornell.edu/courses/cs4300/2020sp/Demos/demo05.html).\n",
    "\n",
    "The Merge Postings Algorithm we implemented can be thought of a boolean search with the `AND` operator. Write a function `boolean_search` that implements a similar algorithm with the `NOT` operator using the inverted index.\n",
    "\n",
    "**Note:** Make sure you convert the `query_word` and `not_word` to lowercase. \n",
    "\n",
    "\n",
    "------------------------------------------\n",
    "    Initialize empty list (called merged list M)\n",
    "\n",
    "    Create sorted list A of documents containing the query_word\n",
    "\n",
    "    Create sorted list B of documents containing the not_word\n",
    "\n",
    "    Start: Pointer at the first element of both A and B\n",
    "\n",
    "    Do: Does it point to the same document ID in each list?\n",
    "\n",
    "        Yes: advance pointer in both A and B\n",
    "    \n",
    "        No: \n",
    "            If the pointer with the smaller document ID is in list A:\n",
    "                Append the smaller document ID to list M\n",
    "            \n",
    "            Advance (to the right) the pointer with the smaller ID\n",
    "    \n",
    "    End: When we attempt to advance a pointer already at the end of its list\n",
    "\n",
    "    Finally: if there are remaining document IDs in list A that were not evaluated in the above loop, then append them to list M.\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "**Note:** The objective is to demonstrate your knowledge in building an efficient search algorithm. If you use the Python `set.difference` function, you will lose points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f72c771deef339673634f82c98d2148",
     "grade": false,
     "grade_id": "boolean_search",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def boolean_search(query_word,not_word, inverted_index):\n",
    "    \"\"\" Search the collection of documents for the given query_word \n",
    "        provided that the documents do not include the not_word\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query_word: string,\n",
    "        The word we are searching for in our documents.\n",
    "    \n",
    "    not_word: string,\n",
    "        The word excluded from our documents.\n",
    "    \n",
    "    index: an inverted index as above\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    results: list of ints\n",
    "        Sorted List of results (in increasing order) such that every element is a `doc_id`\n",
    "        that points to a document that satisfies the boolean\n",
    "        expression of the query.\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    postings1 = [ i for (i,j) in inverted_index[query_word.lower()]]\n",
    "    postings2 = [ i for (i,j) in inverted_index[not_word.lower()]]\n",
    "    \n",
    "    diff_posting=[ i for (i,j) in inverted_index[query_word.lower()]]\n",
    "    i,j=0,0\n",
    "    while i<len(postings1) and j<len(postings2):\n",
    "        if postings1[i]==postings2[j]:\n",
    "            diff_posting.remove(postings1[i])\n",
    "            i+=1\n",
    "            j+=1\n",
    "        elif postings1[i]<postings2[j]:\n",
    "            i+=1\n",
    "        else:\n",
    "            j+=1\n",
    "            \n",
    "    return diff_posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1882633e5ee3bd65381e16cf786b8d04",
     "grade": true,
     "grade_id": "boolean_search_test",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "result0_start_time = time.time()\n",
    "result0 = boolean_search('ice','cream', inv_idx)\n",
    "result0_execution_time = time.time() - result0_start_time\n",
    "result3 = boolean_search('puppy','dog', inv_idx)\n",
    "result1= boolean_search('Kardashian','Kim',inv_idx)\n",
    "result4= boolean_search('cake','cake',inv_idx)\n",
    "assert result0_execution_time < 1.0\n",
    "assert type(result1) == list\n",
    "assert len(result3) == 7\n",
    "assert len(result4)==0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "785c8d855326f5de3da9d929ca2e1e79",
     "grade": false,
     "grade_id": "cell-f8a80e02cfb14fc4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q2b Using the inverted index for boolean search (Free Response)\n",
    "\n",
    "In A3 we already explored search techniques which are able to find a wider variety of relevant results. Why might you want to use a boolean search with an inverted index instead? Give a specific example in which a boolean search would be a better choice than a search with edit distance, and justify why a boolean search would be preferable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6698b5447fbcba245c0b701a865ee70",
     "grade": false,
     "grade_id": "cell-7975337e0b58caa8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c388d41a9422fbbd92aeab9dd7278fc7",
     "grade": true,
     "grade_id": "boolean_search_ans",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "The benefits of using boolean search: \n",
    "\n",
    "1. It is much faster than edit distance\n",
    "2. It gives you the relevent results with the words you want to emphasize.\n",
    "\n",
    "**For example, if you use edit distance, but there are a lot of words that are same but not what you really care about, ie, not relevent to your query.**\n",
    "\n",
    "Query: i do not like breakfast because i do not like sausages.\n",
    "\n",
    "Doc2: i do not like break because i do not like my age.\n",
    "\n",
    "Doc3: i hate breakfast because i never eat sausage.\n",
    "\n",
    "when you try to calculate the edit distance, you may notice that doc2 and query are similar than doc3 and query, because, there are a lot of same unrelevent words (like \"i, do, not, like\"). However, in the boolean search, if we want the words: breakfast and sausages appear together, then doc3 is the most relevent and similar documents for this query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36617fb84a0ff5ca37d43a9754ce5e2d",
     "grade": false,
     "grade_id": "cell-1ad52f18c9e1026b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-top: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold; text-align: center;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df3a74d3829a3e063ab178434cdae083",
     "grade": false,
     "grade_id": "cell-1d10f0b8dc4ffc19",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q3 Compute IDF *using* the inverted index (Code Completion)\n",
    "\n",
    "Write a function `compute_idf` that uses the inverted index to efficiently compute IDF values.\n",
    "\n",
    "Words that occur in a very small number of documents are not useful in many cases, so we ignore them. Use a parameter `min_df`\n",
    "to ignore all terms that occur in strictly fewer than `min_df=10` documents.\n",
    "\n",
    "Similarly, words that occur in a large *fraction* of the documents don't bring any more information for some tasks. Use a parameter `max_df_ratio` to trim out such words. For example, `max_df_ratio=0.95` means ignore all words that occur in more than 95% of the documents.\n",
    "\n",
    "As a reminder, we define the IDF statistic as...\n",
    "$$ IDF(t) = \\log \\left(\\frac{N}{1 + DF(t)} \\right) $$\n",
    "\n",
    "where $N$ is the total number of docs and $DF(t)$ is the number of docs containing $t$. Keep in mind, there are other definitions if IDF out there, so if you go looking for resources on the internet, you might find differing (but also valid) accounts. In practice the base of the log doesn't really matter, however you should use base 2 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab6828ded82a44cdac7c436e89e39cb9",
     "grade": false,
     "grade_id": "compute_idf",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_idf(inv_idx, n_docs, min_df=10, max_df_ratio=0.95):\n",
    "    \"\"\" Compute term IDF values from the inverted index.\n",
    "    Words that are too frequent or too infrequent get pruned.\n",
    "    \n",
    "    Hint: Make sure to use log base 2.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    inv_idx: an inverted index as above\n",
    "    \n",
    "    n_docs: int,\n",
    "        The number of documents.\n",
    "        \n",
    "    min_df: int,\n",
    "        Minimum number of documents a term must occur in.\n",
    "        Less frequent words get ignored. \n",
    "        Documents that appear min_df number of times should be included.\n",
    "    \n",
    "    max_df_ratio: float,\n",
    "        Maximum ratio of documents a term can occur in.\n",
    "        More frequent words get ignored.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    idf: dict\n",
    "        For each term, the dict contains the idf value.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    d = defaultdict(float)\n",
    "    for word, value in inv_idx.items():\n",
    "        count = len(value)\n",
    "        if (count >=min_df and count<=n_docs*max_df_ratio):\n",
    "            d[word] = round(np.log2(n_docs/(1+count)),2)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77bab12b5ea13930f9ec1a91a3573609",
     "grade": true,
     "grade_id": "compute_idf_test",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "start_time = time.time()\n",
    "idf_dict = compute_idf(inv_idx, len(flat_msgs))\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert len(idf_dict) < len(inv_idx)\n",
    "assert 'blah' not in idf_dict\n",
    "assert 'blah' in inv_idx \n",
    "assert '.' in idf_dict\n",
    "assert '3' not in idf_dict\n",
    "assert idf_dict['bruce'] >= 6.0 and idf_dict['bruce'] <= 7.0\n",
    "assert idf_dict['baby'] >= 6.0 and idf_dict['baby'] <= 8.0\n",
    "assert execution_time <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f22c3614bb0bd1c33020845f49729a7",
     "grade": false,
     "grade_id": "cell-976a9144b11db274",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q4 Compute the norm of each document using the inverted index (Code Completion)\n",
    "\n",
    "Recalling our tf-idf vector representation of documents, we can compute the \"norm\" as the norm (length) of the vector representation of that document. More specifically, the norm of a document $j$, denoted as $\\left|\\left| d_j \\right|\\right|$, is given as follows...\n",
    "\n",
    "$$ \\left|\\left| d_j \\right|\\right| = \\sqrt{\\sum_{\\text{word } i} (tf_{ij} \\cdot idf_i)^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "881f76fc025a8f6d4c5c516471c553db",
     "grade": false,
     "grade_id": "compute_doc_norms",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_doc_norms(index, idf, n_docs):\n",
    "    \"\"\" Precompute the euclidean norm of each document.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    index: the inverted index as above\n",
    "    \n",
    "    idf: dict,\n",
    "        Precomputed idf values for the terms.\n",
    "    \n",
    "    n_docs: int,\n",
    "        The total number of documents.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    norms: np.array, size: n_docs\n",
    "        norms[i] = the norm of document i.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    arr = np.zeros(n_docs)\n",
    "\n",
    "    for word, ridx in index.items():\n",
    "        for doc_id, count in ridx:\n",
    "            try:\n",
    "                arr[doc_id] += count*idf[word] ** 2\n",
    "            except KeyError:\n",
    "                pass\n",
    "    arr = arr ** 0.5\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69f8f675f4a3f8a07b202bbb8c0bbf7e",
     "grade": true,
     "grade_id": "compute_doc_norms_test",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "start_time = time.time()\n",
    "doc_norms = compute_doc_norms(inv_idx, idf_dict, len(flat_msgs))\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert len(flat_msgs) == len(doc_norms)\n",
    "assert doc_norms[3722] == 0\n",
    "assert max(doc_norms) < 80\n",
    "assert doc_norms[1] >= 15.5 and doc_norms[1] <= 17.5\n",
    "assert doc_norms[5] >= 6.5 and doc_norms[5] <= 8.5\n",
    "assert execution_time <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c50c75eab21e82d719f2f37d4d4ebed4",
     "grade": false,
     "grade_id": "cell-7fef1edededcee58",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q5 Find the most similar messages to the quotes (Code Completion)\n",
    "\n",
    "The goal of this section is to implement `index_search`, a fast implementation of cosine similarity. You will then test your answer by running the search function using the code provided. Briefly discuss why it worked, or why it might not have worked, for each query.\n",
    "\n",
    "The goal of `index_search` is to compute the cosine similarity between the query and each document in the dataset. Naively, this computation requires you to compute dot products between the query tf-idf vector $q$ and each document's tf-idf vector $d_i$.\n",
    "\n",
    "However, you should be able to use the sparsity of the tf-idf representation and the data structures you created to your advantage. More specifically, consider the cosine similarity...\n",
    "\n",
    "$$ cossim(\\vec{q}, \\vec{d_j}) = \\frac{\\vec{q} \\cdot \\vec{d_j}}{\\|\\vec{q}\\| \\cdot \\|\\vec{d_j}\\|}$$\n",
    "\n",
    "Specifically, focusing on the numerator...\n",
    "\n",
    "$$ \\vec{q} \\cdot \\vec{d_j} = \\sum_{i} {q_i} * {d_i}_j $$\n",
    "\n",
    "Here ${q_i}$ and ${d_i}_j$ are the $i$-th dimension of the vectors $q$ and ${d_j}$ respectively.\n",
    "Because many ${q_i}$ and ${d_i}_j$ are zero, it is actually a bit wasteful to actually create the vectors $q$ and $d_j$ as numpy arrays; this is the method that you saw in class.\n",
    "\n",
    "A faster approach to computing the numerator term of cosine similarity involves quickly computing the above summation using the inverted index, pre-computed idf scores, and pre-computed document norms.\n",
    "\n",
    "A good \"first step\" to implementing this efficiently is to only loop over ${q}_j$ that are nonzero (i.e. ${q}_j$ such that the word $j$ appears in the query). \n",
    "\n",
    "**Note:** Convert the query to lowercase, and use the `nltk.tokenize.TreebankWordTokenizer` to tokenize the query (provided to you as the `tokenizer` parameter). The transcripts have already been tokenized this way. <br>\n",
    "\n",
    "**Note 2:** For `index_search`, you need not remove punctuation tokens from the tokenized query before searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db2a7d00b1efd4658e30a5d83a5763da",
     "grade": false,
     "grade_id": "cell-b39f99a59a119d67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Aside:** Precomputation\n",
    "\n",
    "In many settings, we will need to repeat the same kind of operation many times. Often, part of the input doesn't change.\n",
    "Queries against the Kardashians transcript are like this: we want to run more queries (in the real world we'd want to run a lot of them every second, even) but the data we are searching doesn't change.\n",
    "\n",
    "We could write an `index_search` function with the same signature as A3's `verbatim_search`, taking the `query` and the `msgs` as input, and the function would look like:\n",
    "\n",
    "    def index_search(query, msgs):\n",
    "        inv_idx = build_inverted_index(msgs)\n",
    "        idf = compute_idf(inv_idx, len(msgs))\n",
    "        doc_norms = compute_doc_norms(inv_idx)\n",
    "        # do actual search\n",
    "\n",
    "\n",
    "But notice that the first three lines only depend on the messages. Imagine if we run this a million times with different queries but the same collection of documents: we'd wastefully recompute the index, the IDFs and the norms every time and discard them. It's a better idea, then, to precompute them just once, and pass them as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94f16111f94539105aa62a3cc75c7907",
     "grade": false,
     "grade_id": "cell-9f5e36e0536c8e91",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "inv_idx = build_inverted_index(flat_msgs)\n",
    "\n",
    "idf = compute_idf(inv_idx, len(flat_msgs),\n",
    "                  min_df=10,\n",
    "                  max_df_ratio=0.1)  # documents are very short so we can use a small value here\n",
    "                                     # examine the actual DF values of common words like \"the\"\n",
    "                                     # to set these values\n",
    "\n",
    "inv_idx = {key: val for key, val in inv_idx.items()\n",
    "           if key in idf}            # prune the terms left out by idf\n",
    "\n",
    "doc_norms = compute_doc_norms(inv_idx, idf, len(flat_msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12feca27ee5d4846682594ad67b59fd3",
     "grade": false,
     "grade_id": "index_search",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def index_search(query, index, idf, doc_norms, tokenizer=treebank_tokenizer):\n",
    "    \"\"\" Search the collection of documents for the given query\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query: string,\n",
    "        The query we are looking for.\n",
    "    \n",
    "    index: an inverted index as above\n",
    "    \n",
    "    idf: idf values precomputed as above\n",
    "    \n",
    "    doc_norms: document norms as computed above\n",
    "    \n",
    "    tokenizer: a TreebankWordTokenizer\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    results, list of tuples (score, doc_id)\n",
    "        Sorted list of results such that the first element has\n",
    "        the highest score, and `doc_id` points to the document\n",
    "        with the highest score.\n",
    "    \n",
    "    Note: \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    query = query.lower()\n",
    "    query_toks = tokenizer.tokenize(query)\n",
    "    query_counter = Counter(query_toks)\n",
    "\n",
    "    \n",
    "    query_norm =0\n",
    "    for word, count in query_counter.items():\n",
    "        query_norm += np.power(count*idf[word],2)\n",
    "    query_norm  = np.sqrt(query_norm)\n",
    "\n",
    "\n",
    "    dot_list=np.zeros(len(doc_norms))\n",
    "    for word, count in query_counter.items():\n",
    "        if word in index.keys():\n",
    "            tuple_list = index[word]\n",
    "            for tup in tuple_list:\n",
    "                dot_list[tup[0]] += count*tup[1]*idf[word]*idf[word]\n",
    "\n",
    "    dot_n, doc_n, idx = [], [], []\n",
    "    for i in range(len(doc_norms)):\n",
    "        if doc_norms[i] != 0:\n",
    "            dot_n.append(dot_list[i])\n",
    "            doc_n.append(doc_norms[i])\n",
    "            idx.append(i)\n",
    "    score_list = np.array(dot_n)/(np.array(doc_n) * query_norm)\n",
    "\n",
    "    final = [(score_list[i], i) for i in range(len(score_list))]\n",
    "    \n",
    "    final_sorted = sorted(final, key = lambda x: x[0], reverse = True)\n",
    "\n",
    "    return final_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76b2420b3375e90b666ca99972bd55df",
     "grade": true,
     "grade_id": "index_search_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################\n",
      "It's like a bunch of people running around talking about nothing.\n",
      "#################################################################\n",
      "[1.00] KRIS: They brought me over here so I could talk to you.\n",
      "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
      "[0.61] BRUCE: Really?\n",
      "\t(Keeping Up With the Kardashians - Kris ``The Cougar'' Jenner)\n",
      "[0.46] KIM: Can you play catch over there?\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.46] KOURTNEY: Do you think I'm going to help you if talk to me like that?\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "[0.43] BRUCE: Yeah.\n",
      "\t(Keeping Up With the Kardashians - Botox and Cigarettes)\n",
      "[0.43] LAUREN: I'm just trying to help her.\n",
      "\t(Keeping Up With the Kardashians - What's Yours Is Mine)\n",
      "[0.43] LAUREN: \"Oh, my God, I'm a fan.\n",
      "\t(Keeping Up With the Kardashians - What's Yours Is Mine)\n",
      "[0.42] KIM: I'm just afraid if you go to New York, you'll come back hurt.\"- it.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.42] BRUCE: What?\n",
      "\t(Keeping Up With the Kardashians - Must Love Dogs)\n",
      "[0.41] KRIS: And maybe I've gone a little too far with this.\n",
      "\t(Keeping Up With the Kardashians - The Missing Ring)\n",
      "\n",
      "#############################################################################################\n",
      "Never say to a famous person that this possible endorsment would bring 'er to the spot light.\n",
      "#############################################################################################\n",
      "[0.44] KIM: plemulti ..\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.44] KHLOE: Well, she did have six kids; I had none.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "[0.44] KRIS: That is not very nice.\n",
      "\t(Keeping Up With the Kardashians - Body Blows)\n",
      "[0.43] KOURTNEY: That's not all.\n",
      "\t(Keeping Up With the Kardashians - All for One and One for Kim)\n",
      "[0.40] KIM: I'm not up own my ass-- I'm just really busy doing stuff.\n",
      "\t(Keeping Up With the Kardashians - Kim Becomes a Diva)\n",
      "[0.39] MARIO: Forgive me for laughing.\n",
      "\t(Keeping Up With the Kardashians - Leaving the Nest)\n",
      "[0.38] KIM: Pledge multi surfaceis fast and effective.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.38] DOCTOR: Hello.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "[0.38] KIM: This was supposed to be me!\" Like, I think, you know, like, what if I have a nervous breakdown when they're like, \"Does anyone object?\" What if I'm, like, \"This was supposed to be me!\" Like in a dream.\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[0.38] KIM: Thank you.\n",
      "\t(Keeping Up With the Kardashians - Body Blows)\n",
      "\n",
      "####################################\n",
      "Your yapping is making my head ache!\n",
      "####################################\n",
      "[0.69] WOMAN: Hi.\n",
      "\t(Keeping Up With the Kardashians - Delivering Baby Mason)\n",
      "[0.56] KOURTNEY: Bruce is like an active guy, and he would rather do active stuff.\n",
      "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
      "[0.56] KOURTNEY: All right, I'm going to do some filing.\n",
      "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
      "[0.56] KRIS: Got it.\n",
      "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
      "[0.56] FRANKIE: Uh-oh.\n",
      "\t(Keeping Up With the Kardashians - Brody in the House)\n",
      "[0.53] KRIS: How are you feeling?\n",
      "\t(Keeping Up With the Kardashians - The Two Year Itch)\n",
      "[0.52] MAN: Nice... and cut.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "[0.52] WOMAN: You look so pretty.\n",
      "\t(Keeping Up With the Kardashians - Delivering Baby Mason)\n",
      "[0.52] KIM: I feel really bad for Kourt.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "[0.52] BRUCE: He's not father material.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "\n",
      "######################################\n",
      "I'm going to Maryland, did I tell you?\n",
      "######################################\n",
      "[1.00] KIM: You have to be in sync every step of the way.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.74] BRUCE: Give me the phone.\n",
      "\t(Keeping Up With the Kardashians - Meet the Kardashians)\n",
      "[0.70] KHLOE: Shall we call Kourt?\n",
      "\t(Keeping Up With the Kardashians - Body Blows)\n",
      "[0.61] KHLOE: Hello, Smooch.\n",
      "\t(Keeping Up With the Kardashians - Khloe Wants to Act)\n",
      "[0.60] BRUCE: I mean, you would know.\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "[0.60] KHLOE: Mm-hmm.\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "[0.58] BRUCE: 447-Charlie-1292.\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "[0.57] KRIS: I just stopped a wedding.\n",
      "\t(Keeping Up With the Kardashians - You Are So Pregnant Dude)\n",
      "[0.56] KHLOE: So you shouldn't take yazif you have kidney, liver, or adrenal diseasebecause this could ..\n",
      "\t(Keeping Up With the Kardashians - Khloe's Blind Dates)\n",
      "[0.56] KOURTNEY: Are you supposed to feed it those kind of cookies?\n",
      "\t(Keeping Up With the Kardashians - Shape Up or Ship Out)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "start_time = time.time()\n",
    "results = index_search(queries[1], inv_idx, idf, doc_norms)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert type(results[0]) == tuple\n",
    "assert max(results)[0] == results[0][0]\n",
    "assert results[0][0] >= 0.4 and results[0][0] <= 0.48\n",
    "assert execution_time <= 1.0\n",
    "\n",
    "\n",
    "for query in queries:\n",
    "    print(\"#\" * len(query))\n",
    "    print(query)\n",
    "    print(\"#\" * len(query))\n",
    "\n",
    "    for score, msg_id in index_search(query, inv_idx, idf, doc_norms)[:10]:\n",
    "        print(\"[{:.2f}] {}: {}\\n\\t({})\".format(\n",
    "            score,\n",
    "            flat_msgs[msg_id]['speaker'],\n",
    "            flat_msgs[msg_id]['text'],\n",
    "            flat_msgs[msg_id]['episode_title'])) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45b461ded5f437850ac0958ecf30cff2",
     "grade": false,
     "grade_id": "cell-f79bce72b4282bde",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q5b Find the most similar messages to the quotes (Free Response)\n",
    "\n",
    "Briefly discuss why cosine similarity worked, or why it might not have worked, **for each query**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c1e5d7e0052ed58c33c9fb5576ccc54",
     "grade": false,
     "grade_id": "cell-7b71a29cbc6a0190",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "174a55d92226ba5fa3e6bd98cf424b50",
     "grade": true,
     "grade_id": "index_search_ans",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "1. Some of doc_norms[i] maybe zeros, because when we calculate idfs, we ignore all words that occur in more than 95% of the documents and occurs less than 10. Therefore, if a document contains all the words that have been ingored by us, then the denominator could be 0, in this case, we cannot compare this documents this the query. We have to ignore the whole sentence. \n",
    "\n",
    "2. Except for this case, all other sentences could compare with the query, as some the unnecessary words have been ignored by the filter. The comparison score result would be better than edit distance seach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef5221b2f70c421bf0a9a34aa8cad8a0",
     "grade": false,
     "grade_id": "cell-40593bc03404aacd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-top: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold; text-align: center;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "233af6ed60b83b9b686dff616d651ced",
     "grade": false,
     "grade_id": "cell-6cbbf6d5e1dd4da8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q6EC: Extra credit question 1 (optional)\n",
    "\n",
    "### Updating precomputed values.\n",
    "\n",
    "In many real-world applications, the collection of documents will not stay the same forever. At Internet-scale, however, it could possibly even be worth recomputing things every second, if during that second we're going to answer millions of queries.\n",
    "\n",
    "However, there's a better way: in reality, the document set will not change radically, but incrementally.  In particular, it's most common to add or remove a bunch of new documents to the index.\n",
    "\n",
    "Write functions `add_docs` and `remove_docs` that update the index, idf and document norms.  Think of the implications this has on how we store the IDF. Is there a better way of storing it, that minimizes the memory we need to touch when updating?\n",
    "\n",
    "Think of adequate test cases for these functions and implement them.\n",
    "\n",
    "**Note:** You can get up to 0.5 EC for completing this question. *Do not delete the cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a44b8ec72f9b03c736614d5c478a6e9",
     "grade": true,
     "grade_id": "extra_credit_1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def add_docs(docs, index, msgs, tokenizer=treebank_tokenizer):\n",
    "    # update the index\n",
    "    for doc in docs:\n",
    "        doc = doc.lower()\n",
    "        doc_toks = tokenizer.tokenize(doc)\n",
    "        doc_counter = Counter(doc_toks)\n",
    "\n",
    "        # update index:\n",
    "        for word, count in doc_counter.items():\n",
    "            index[word].append((len(msgs), count))\n",
    "\n",
    "        \n",
    "        msgs.append({'text': doc, 'toks': doc_toks})\n",
    "    idf = compute_idf(index, len(msgs))\n",
    "    doc_norm = compute_doc_norms(index, idf, len(msgs))\n",
    "    \n",
    "    return idf, doc_norm\n",
    "    \n",
    "\n",
    "def remove_docs(docs_number, index, msgs):\n",
    "    # update index:\n",
    "    new_index = index.copy()\n",
    "    for num in docs_number:\n",
    "        for word, value in new_index.items():\n",
    "            for tup in value:\n",
    "                if tup[0]==num:\n",
    "                    index[word].remove(tup)\n",
    "                               \n",
    "    idf = compute_idf(index, len(msgs))\n",
    "    doc_norm = compute_doc_norms(index, idf, len(msgs))\n",
    "                               \n",
    "    return idf, doc_norm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "docs = ['You have to be in sync every step of the way.', 'You have to be in sync every step of the way.', \n",
    "        'I\\'m not up own my ass-- I\\'m just really busy doing stuff.']\n",
    "# since the index will be updated, therefore, we need to record the original value before head.\n",
    "l1 = len(inv_idx['ass'])\n",
    "l2 = len(inv_idx['step'])\n",
    "\n",
    "# now, it is time to run the method(ie. operation)\n",
    "start_time = time.time()\n",
    "add_docs(docs, inv_idx, flat_msgs,tokenizer=treebank_tokenizer)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "assert len(inv_idx) <= 10000 \n",
    "assert len(inv_idx['ass']) == l1 + 1\n",
    "assert len(inv_idx['step']) == l2 + 2\n",
    "assert execution_time <= 1.0\n",
    "\n",
    "# after the update, the index now is the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39680]\n",
      "74\n",
      "148\n",
      "73\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "docs = [len(flat_msgs)-1]\n",
    "l1 = len(inv_idx['ass'])\n",
    "l2 = len(inv_idx['stuff'])\n",
    "\n",
    "# now, it is time to run the method(ie. operation)\n",
    "start_time = time.time()\n",
    "idf, doc_norm = remove_docs(docs, inv_idx, flat_msgs)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "\n",
    "assert len(inv_idx) <= 10000 \n",
    "assert len(inv_idx['ass']) == l1 - 1\n",
    "assert len(inv_idx['stuff']) == l2 - 1\n",
    "assert execution_time <= 1.0\n",
    "# after the update, the index now is the new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9862409006dedc816b74d0198953a57e",
     "grade": false,
     "grade_id": "cell-bbbf1b05f86579fd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q7EC: Extra credit question 2 (optional)\n",
    "\n",
    "### Finding your own similarity metric\n",
    "\n",
    "We've explored using cosine similarity and edit distance to find similar messages to input queries. However, there's a whole world of ways to measure the similarity between two documents. Go forth, and research!\n",
    "\n",
    "(Fun fact: Fundamental information retrieval techniques were in fact developed at Cornell, so you would not be the first Cornellian to disrupt the field)\n",
    "\n",
    "For this question, find a new way of measuring similarity between two documents, and implement a search using your new metric. Your new way of measuring document similarity should be different enough from the two approaches we already implemented. It can be a method you devise or an existing method from somewhere else (make sure to reveal your sources).\n",
    "\n",
    "**Note:** The amount of EC awarded for this question will be determined based on creativity, originality, implementation, and analysis. *Do not delete the cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install gensim\n",
    "!conda install -cy conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6256ee60a590b4e555a167d189281028",
     "grade": true,
     "grade_id": "extra_credit_2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "def word2vec_sumVector_similarity(transcript, query, vector_size):\n",
    "    text = []\n",
    "    for tran in transcript:\n",
    "        for dic in tran:\n",
    "            text.append(dic['toks'])\n",
    "    query_toks = nltk.tokenize.TreebankWordTokenizer().tokenize(query)\n",
    "\n",
    "    text.append(query_toks)\n",
    "\n",
    "    model = Word2Vec(text, min_count=0,size= vector_size, workers=5, window =3, sg = 1)\n",
    "    \n",
    "    score = []\n",
    "    i=0\n",
    "    arr1 = np.zeros(vector_size)\n",
    "    for word in query_toks:\n",
    "        arr1 += model.wv.__getitem__(word)\n",
    "    for tran in transcript:\n",
    "        for dic in tran:\n",
    "            arr2 = np.zeros(vector_size)\n",
    "            for word in dic['toks']:\n",
    "                arr2 += model.wv.__getitem__(word)\n",
    "    \n",
    "    # next is cosine similarity:\n",
    "            arr_difference = np.sum(np.absolute(arr1-arr2))\n",
    "            i=i+1\n",
    "            score.append((arr_difference, i))\n",
    "    score_sorted = sorted(score, key = lambda x: x[0], reverse = True)  \n",
    "    \n",
    "    return score_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = doc2vec_sumVector_similarity(transcripts, queries[1], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1196.378324575504, 38367),\n",
       " (904.1096134463733, 23392),\n",
       " (904.1096134463733, 30914),\n",
       " (831.5588263881509, 12205),\n",
       " (813.6027307417535, 24099),\n",
       " (813.6027307417535, 31630),\n",
       " (796.6861806639936, 8620),\n",
       " (705.1169240947929, 19058),\n",
       " (689.4845316393767, 22982),\n",
       " (689.4845316393767, 30486),\n",
       " (658.2664172488148, 17615),\n",
       " (651.5606833014754, 26482),\n",
       " (625.6189396629925, 4882),\n",
       " (618.2968378780643, 23004),\n",
       " (618.2968378780643, 30508),\n",
       " (617.2576028854528, 22947),\n",
       " (617.2576028854528, 30451),\n",
       " (600.4785345961573, 24095),\n",
       " (600.4785345961573, 31626),\n",
       " (595.7245208604436, 29926),\n",
       " (595.7245208604436, 30395),\n",
       " (591.7070633434632, 19834),\n",
       " (573.0796258652408, 34304),\n",
       " (569.2535770292452, 34631),\n",
       " (558.5888929793, 8794),\n",
       " (551.2901567779481, 33177),\n",
       " (548.1896139680757, 16037),\n",
       " (548.1896139680757, 16598),\n",
       " (546.3483432164066, 39149),\n",
       " (533.0017258305015, 11747),\n",
       " (531.8071531411842, 9219),\n",
       " (525.582943109097, 13830),\n",
       " (524.8601823651697, 14083),\n",
       " (524.7072964233812, 22343),\n",
       " (523.4293034612783, 37568),\n",
       " (514.7343128589564, 28118),\n",
       " (510.7242921263678, 10663),\n",
       " (509.5713391049794, 692),\n",
       " (509.5713391049794, 1201),\n",
       " (507.1947642507148, 18844),\n",
       " (505.95574632729404, 8422),\n",
       " (493.42737083926477, 38427),\n",
       " (493.3511482789618, 29927),\n",
       " (493.3511482789618, 30396),\n",
       " (486.9400660484389, 19508),\n",
       " (483.4386695192661, 24357),\n",
       " (483.4386695192661, 31914),\n",
       " (482.9988529891125, 27805),\n",
       " (479.7546799231204, 12531),\n",
       " (470.3325025458471, 9204),\n",
       " (463.11837730427214, 8942),\n",
       " (456.9339872545388, 8833),\n",
       " (455.33772764240166, 7568),\n",
       " (455.33772764240166, 36521),\n",
       " (454.92398448934546, 24118),\n",
       " (454.92398448934546, 31649),\n",
       " (444.1660190052935, 660),\n",
       " (444.1641775778844, 38233),\n",
       " (440.63766248105094, 23299),\n",
       " (440.63766248105094, 30821),\n",
       " (433.01947420989745, 13827),\n",
       " (432.7827927086764, 29957),\n",
       " (432.7827927086764, 30426),\n",
       " (432.64007415942615, 28374),\n",
       " (431.195554458478, 38774),\n",
       " (431.195554458478, 39388),\n",
       " (431.0731735043955, 16092),\n",
       " (431.0731735043955, 16653),\n",
       " (427.13852434576256, 14497),\n",
       " (426.8951438976801, 18051),\n",
       " (426.7979653571092, 36587),\n",
       " (425.0559880785295, 13941),\n",
       " (424.1236865450337, 17599),\n",
       " (423.73431129503297, 28319),\n",
       " (421.71114102800493, 26459),\n",
       " (420.213519625715, 3886),\n",
       " (420.213519625715, 18587),\n",
       " (417.05238649834064, 3311),\n",
       " (413.68055043611093, 25265),\n",
       " (413.2840696007188, 12938),\n",
       " (411.63462330342736, 23731),\n",
       " (411.63462330342736, 31255),\n",
       " (411.1361606406863, 10177),\n",
       " (408.64718331891345, 39247),\n",
       " (407.8945810988662, 35373),\n",
       " (407.625476303685, 15543),\n",
       " (404.73410466763744, 37416),\n",
       " (403.89326566189993, 28350),\n",
       " (401.1021246644523, 25805),\n",
       " (400.7412823729246, 35540),\n",
       " (400.263525251532, 16373),\n",
       " (399.5506372208765, 23786),\n",
       " (399.5506372208765, 31310),\n",
       " (398.4720056744991, 36024),\n",
       " (397.8116767416359, 9633),\n",
       " (396.2047446081997, 3950),\n",
       " (396.2047446081997, 18645),\n",
       " (396.1817599161586, 24071),\n",
       " (396.1817599161586, 31598),\n",
       " (394.99392157880357, 28153),\n",
       " (393.9024636373215, 29891),\n",
       " (393.9024636373215, 30360),\n",
       " (391.9809128601264, 28371),\n",
       " (390.7139810125809, 13751),\n",
       " (390.5660774162534, 8868),\n",
       " (390.1403202522415, 7488),\n",
       " (389.51221075456124, 695),\n",
       " (388.9678770334576, 22940),\n",
       " (388.9678770334576, 30444),\n",
       " (388.7987831232458, 13551),\n",
       " (388.3474061109591, 34628),\n",
       " (388.1057166373939, 17766),\n",
       " (387.8592615519301, 15826),\n",
       " (387.8592615519301, 16379),\n",
       " (387.81946516080643, 22803),\n",
       " (386.51336244511185, 11651),\n",
       " (386.29902094538556, 3939),\n",
       " (386.29902094538556, 18634),\n",
       " (384.6043072537868, 5491),\n",
       " (384.6043072537868, 30099),\n",
       " (383.81194690905977, 19044),\n",
       " (383.4919515391957, 15320),\n",
       " (382.07918696942215, 38060),\n",
       " (381.4098704569042, 13068),\n",
       " (381.1930995947914, 1204),\n",
       " (380.2793967704638, 19438),\n",
       " (379.57602126132406, 15946),\n",
       " (379.57602126132406, 16509),\n",
       " (378.9968427543645, 17223),\n",
       " (378.7251322916709, 24183),\n",
       " (378.7251322916709, 31722),\n",
       " (378.71765114582377, 17432),\n",
       " (377.6069578760362, 14640),\n",
       " (377.2349661428889, 17918),\n",
       " (376.2947038982529, 16935),\n",
       " (375.2477137515525, 27845),\n",
       " (374.18952334120695, 39158),\n",
       " (372.1720937304781, 673),\n",
       " (372.1720937304781, 1188),\n",
       " (371.3852058601915, 22989),\n",
       " (371.3852058601915, 30493),\n",
       " (369.6015567329887, 24269),\n",
       " (369.6015567329887, 31820),\n",
       " (369.0277063079411, 13453),\n",
       " (368.49958906072425, 3887),\n",
       " (368.49958906072425, 18588),\n",
       " (367.99140542531677, 7203),\n",
       " (366.96130388087477, 16076),\n",
       " (366.96130388087477, 16637),\n",
       " (366.71197292579745, 13020),\n",
       " (366.3716835936066, 25456),\n",
       " (366.36247933548293, 32000),\n",
       " (366.1428970610141, 8320),\n",
       " (365.1659538255335, 28211),\n",
       " (363.93703045818256, 19537),\n",
       " (362.8222357427585, 22941),\n",
       " (362.8222357427585, 30445),\n",
       " (362.7873130659136, 36352),\n",
       " (362.3602935511153, 7265),\n",
       " (362.0134953953675, 11766),\n",
       " (360.8959432884585, 12981),\n",
       " (360.839380458754, 15850),\n",
       " (360.839380458754, 16413),\n",
       " (360.8031026221579, 17681),\n",
       " (359.2589738399838, 13545),\n",
       " (359.12646950385533, 18752),\n",
       " (358.9526715567845, 36735),\n",
       " (357.5946802001563, 10191),\n",
       " (357.4973663278215, 1728),\n",
       " (357.4973663278215, 2093),\n",
       " (357.4973663278215, 4494),\n",
       " (357.4973663278215, 33379),\n",
       " (357.2440925168339, 33159),\n",
       " (356.56188909491175, 29885),\n",
       " (356.56188909491175, 30354),\n",
       " (356.0581945927406, 23922),\n",
       " (356.0581945927406, 31449),\n",
       " (354.63085684820544, 20295),\n",
       " (354.63085684820544, 20718),\n",
       " (354.63085684820544, 21187),\n",
       " (354.63085684820544, 21656),\n",
       " (354.63085684820544, 22125),\n",
       " (354.53118971639196, 11107),\n",
       " (354.008657343511, 15034),\n",
       " (353.985425231891, 25804),\n",
       " (353.96439630925306, 15130),\n",
       " (353.96439630925306, 36753),\n",
       " (353.8182548463228, 3098),\n",
       " (353.14217121197726, 39252),\n",
       " (352.4541706448945, 20335),\n",
       " (352.4541706448945, 20758),\n",
       " (352.4541706448945, 21227),\n",
       " (352.4541706448945, 21696),\n",
       " (352.4541706448945, 22165),\n",
       " (350.9198036385933, 36607),\n",
       " (350.6782392804307, 13756),\n",
       " (349.3325245571905, 7023),\n",
       " (347.4816172911669, 10403),\n",
       " (347.2045694758999, 8725),\n",
       " (345.90143748151604, 23798),\n",
       " (345.90143748151604, 31322),\n",
       " (344.8260926770163, 10291),\n",
       " (344.52317785692867, 36372),\n",
       " (344.02682700232253, 23463),\n",
       " (344.02682700232253, 30985),\n",
       " (344.02306594810216, 8605),\n",
       " (344.02306594810216, 33861),\n",
       " (344.02306594810216, 34085),\n",
       " (344.02306594810216, 36484),\n",
       " (344.02306594810216, 37921),\n",
       " (343.9786988249689, 23982),\n",
       " (343.9786988249689, 31509),\n",
       " (343.9603086448333, 18018),\n",
       " (342.9193166279001, 11289),\n",
       " (342.7721180803783, 561),\n",
       " (342.50366923888214, 26263),\n",
       " (342.42590422942885, 23894),\n",
       " (342.42590422942885, 31421),\n",
       " (341.20567184709944, 20249),\n",
       " (341.20567184709944, 20672),\n",
       " (341.20567184709944, 21141),\n",
       " (341.20567184709944, 21610),\n",
       " (341.20567184709944, 22079),\n",
       " (341.0593465909333, 34317),\n",
       " (340.1145315171452, 1796),\n",
       " (340.1145315171452, 2117),\n",
       " (339.7778215333965, 25799),\n",
       " (339.34705065970775, 24362),\n",
       " (339.34705065970775, 31919),\n",
       " (337.14814911388385, 6341),\n",
       " (337.1398418319004, 13876),\n",
       " (336.22896554338513, 27779),\n",
       " (336.1407845974318, 34643),\n",
       " (335.7976221215795, 7032),\n",
       " (335.51349896393367, 22983),\n",
       " (335.51349896393367, 30487),\n",
       " (335.1386256394908, 26609),\n",
       " (335.1293890965899, 6893),\n",
       " (332.7419386592519, 10482),\n",
       " (332.2686504870944, 7445),\n",
       " (332.0982435925398, 34627),\n",
       " (331.70246030299313, 6818),\n",
       " (331.60958272469, 24371),\n",
       " (331.60958272469, 31928),\n",
       " (331.2479832336685, 27119),\n",
       " (331.0209370966186, 23742),\n",
       " (331.0209370966186, 31266),\n",
       " (330.78630154317943, 19899),\n",
       " (330.6240319875651, 8947),\n",
       " (330.6240319875651, 37929),\n",
       " (330.5233595578029, 14164),\n",
       " (330.47296426910907, 8421),\n",
       " (330.4298300019, 5342),\n",
       " (330.4298300019, 5834),\n",
       " (330.38219152906095, 28868),\n",
       " (330.37952451827005, 10540),\n",
       " (329.69021671317023, 8367),\n",
       " (329.5256148485496, 20146),\n",
       " (329.5256148485496, 20563),\n",
       " (329.5256148485496, 21024),\n",
       " (329.5256148485496, 21493),\n",
       " (329.5256148485496, 21962),\n",
       " (329.3324788887694, 669),\n",
       " (329.3324788887694, 1184),\n",
       " (329.28515983276884, 3116),\n",
       " (328.0826386077679, 5348),\n",
       " (328.0826386077679, 5840),\n",
       " (326.84426851532044, 4517),\n",
       " (326.84426851532044, 4753),\n",
       " (326.5570550119155, 23083),\n",
       " (326.5570550119155, 30587),\n",
       " (325.3230461012572, 17629),\n",
       " (324.9751613167464, 19463),\n",
       " (324.79276955856767, 28212),\n",
       " (323.313134985161, 24116),\n",
       " (323.313134985161, 31647),\n",
       " (322.8026663220371, 24059),\n",
       " (322.8026663220371, 31586),\n",
       " (322.5745496478776, 22972),\n",
       " (322.5745496478776, 30476),\n",
       " (322.134937643139, 28704),\n",
       " (321.87741622375324, 8481),\n",
       " (321.3687422494404, 17925),\n",
       " (320.6683804584318, 36514),\n",
       " (320.5937299748912, 24355),\n",
       " (320.5937299748912, 31912),\n",
       " (320.2891469229071, 11784),\n",
       " (319.568499665751, 36365),\n",
       " (319.443192950348, 26955),\n",
       " (319.3180871841614, 10301),\n",
       " (318.2096868108347, 19172),\n",
       " (317.3305999300792, 1639),\n",
       " (314.59143682109425, 3702),\n",
       " (314.59143682109425, 4721),\n",
       " (314.0153009467758, 36090),\n",
       " (313.9298775103525, 20293),\n",
       " (313.9298775103525, 20716),\n",
       " (313.9298775103525, 21185),\n",
       " (313.9298775103525, 21654),\n",
       " (313.9298775103525, 22123),\n",
       " (313.1597942683147, 3097),\n",
       " (313.0155742823845, 10705),\n",
       " (312.2162804277614, 2709),\n",
       " (312.0394757859467, 7517),\n",
       " (311.54323609225685, 26561),\n",
       " (311.00582961257896, 3785),\n",
       " (311.00582961257896, 37327),\n",
       " (309.84311608219286, 12830),\n",
       " (309.84311608219286, 34470),\n",
       " (309.71205264676246, 24370),\n",
       " (309.71205264676246, 31927),\n",
       " (309.0424246459297, 10357),\n",
       " (308.0161663920153, 29825),\n",
       " (308.0161663920153, 37321),\n",
       " (307.56995050750265, 11608),\n",
       " (307.4878674824431, 23406),\n",
       " (307.4878674824431, 30928),\n",
       " (307.1894403958577, 59),\n",
       " (307.1894403958577, 889),\n",
       " (307.00190232973546, 24766),\n",
       " (306.25698469704366, 27981),\n",
       " (304.69749429792864, 38203),\n",
       " (304.19716913453885, 9349),\n",
       " (303.73929932696046, 28826),\n",
       " (303.6727622257604, 28954),\n",
       " (303.58708708544873, 38260),\n",
       " (303.5133866246906, 24665),\n",
       " (303.381824633776, 38528),\n",
       " (303.29664516239427, 16978),\n",
       " (302.6934401381877, 7313),\n",
       " (301.9635034781677, 28882),\n",
       " (301.7534110740962, 18940),\n",
       " (301.5111565994448, 13896),\n",
       " (301.45703185279854, 15823),\n",
       " (300.78021070727846, 12861),\n",
       " (300.78021070727846, 34476),\n",
       " (300.78021070727846, 37869),\n",
       " (300.38491365048685, 28550),\n",
       " (299.8292364645167, 5058),\n",
       " (299.57150945188187, 1772),\n",
       " (299.57150945188187, 2113),\n",
       " (299.57150945188187, 4499),\n",
       " (299.57150945188187, 33384),\n",
       " (299.5667789710569, 17576),\n",
       " (299.2722584240837, 24381),\n",
       " (299.2722584240837, 31938),\n",
       " (298.54641507036285, 32978),\n",
       " (298.51056309451815, 6096),\n",
       " (298.38592642547155, 25336),\n",
       " (297.98897141817724, 97),\n",
       " (297.98897141817724, 902),\n",
       " (297.98897141817724, 1278),\n",
       " (297.98897141817724, 1969),\n",
       " (297.98897141817724, 2031),\n",
       " (297.98897141817724, 4279),\n",
       " (297.98897141817724, 4322),\n",
       " (297.98897141817724, 4367),\n",
       " (297.98897141817724, 4443),\n",
       " (297.98897141817724, 33184),\n",
       " (297.98897141817724, 33227),\n",
       " (297.98897141817724, 33272),\n",
       " (297.98897141817724, 33328),\n",
       " (297.27999801044643, 11897),\n",
       " (296.8565724392538, 3878),\n",
       " (296.8565724392538, 18579),\n",
       " (296.2666142593371, 20040),\n",
       " (296.22203531564446, 16925),\n",
       " (295.39945085538784, 13017),\n",
       " (295.3177420448046, 24641),\n",
       " (294.96845836855937, 26287),\n",
       " (294.29050191128044, 15236),\n",
       " (294.17603957789834, 24346),\n",
       " (294.17603957789834, 31903),\n",
       " (294.09279467590386, 28162),\n",
       " (291.74756391212577, 12337),\n",
       " (291.69815664037014, 24533),\n",
       " (291.20429080713075, 19120),\n",
       " (290.95144871561206, 17238),\n",
       " (289.9948867872881, 23097),\n",
       " (289.9948867872881, 30601),\n",
       " (289.86153392384585, 11070),\n",
       " (289.723420121285, 35395),\n",
       " (289.3943938333541, 14375),\n",
       " (289.2794877663546, 5374),\n",
       " (289.2794877663546, 5866),\n",
       " (289.1874912264466, 17868),\n",
       " (288.6012335233245, 38150),\n",
       " (288.2909522100963, 2490),\n",
       " (288.1830903129812, 38055),\n",
       " (288.0675694019883, 38098),\n",
       " (287.7530957124254, 10692),\n",
       " (287.66741931039724, 9284),\n",
       " (287.3873986725812, 24914),\n",
       " (287.3873986725812, 32556),\n",
       " (287.3873986725812, 32854),\n",
       " (286.89952901534707, 34844),\n",
       " (285.91379275216605, 19375),\n",
       " (285.7634844726126, 38759),\n",
       " (285.7634844726126, 39374),\n",
       " (285.5606330164592, 37438),\n",
       " (285.097811493848, 18120),\n",
       " (284.9375150826236, 234),\n",
       " (284.70785915106535, 11672),\n",
       " (284.65103702736087, 26232),\n",
       " (284.1420862593659, 35393),\n",
       " (284.0894427768071, 29231),\n",
       " (283.3955999807222, 34000),\n",
       " (283.2146779811592, 36131),\n",
       " (283.05250996001996, 5640),\n",
       " (283.05250996001996, 30247),\n",
       " (282.679286086699, 1825),\n",
       " (282.679286086699, 2137),\n",
       " (282.679286086699, 4503),\n",
       " (282.679286086699, 33388),\n",
       " (282.39992512739263, 13704),\n",
       " (281.54121265758295, 15490),\n",
       " (281.4383826988269, 14889),\n",
       " (281.12171927987947, 12517),\n",
       " (280.7287627237383, 15269),\n",
       " (280.58979837612424, 8837),\n",
       " (280.55476531846216, 20026),\n",
       " (280.52626782126026, 26182),\n",
       " (280.3975739532616, 38787),\n",
       " (280.3975739532616, 39401),\n",
       " (279.6335689013067, 665),\n",
       " (279.4040476081136, 672),\n",
       " (279.3565573479864, 17745),\n",
       " (279.3058681628754, 35810),\n",
       " (279.00082254057634, 25201),\n",
       " (278.5197047284164, 28830),\n",
       " (278.41974058927735, 24792),\n",
       " (278.2091046476271, 28234),\n",
       " (277.99443333261297, 28321),\n",
       " (277.8701344364672, 18165),\n",
       " (277.7348439641064, 14800),\n",
       " (277.48733675929543, 27028),\n",
       " (276.303840246459, 28592),\n",
       " (275.8223515189602, 22791),\n",
       " (275.49696592317196, 10936),\n",
       " (275.49696592317196, 20533),\n",
       " (275.49696592317196, 20994),\n",
       " (275.49696592317196, 21463),\n",
       " (275.49696592317196, 21932),\n",
       " (274.4933956731984, 941),\n",
       " (274.4933956731984, 1286),\n",
       " (274.4933956731984, 1973),\n",
       " (274.4933956731984, 2035),\n",
       " (274.4933956731984, 4282),\n",
       " (274.4933956731984, 4330),\n",
       " (274.4933956731984, 4376),\n",
       " (274.4933956731984, 4447),\n",
       " (274.4933956731984, 33187),\n",
       " (274.4933956731984, 33235),\n",
       " (274.4933956731984, 33275),\n",
       " (274.4933956731984, 33332),\n",
       " (274.47690566041274, 15226),\n",
       " (274.1938325603551, 37356),\n",
       " (273.9974219321157, 34465),\n",
       " (273.9394450372056, 38886),\n",
       " (273.45052129744727, 3490),\n",
       " (272.9214951202157, 36256),\n",
       " (272.62728701805463, 28756),\n",
       " (272.448712278303, 34288),\n",
       " (271.61965356225846, 10435),\n",
       " (271.18907682897407, 13930),\n",
       " (271.15798691270174, 8677),\n",
       " (270.9809176491399, 29509),\n",
       " (270.5516593317152, 2544),\n",
       " (270.53439645358594, 33666),\n",
       " (270.18718413449824, 34652),\n",
       " (270.1814533266588, 24121),\n",
       " (270.1814533266588, 31652),\n",
       " (269.8744404654717, 13423),\n",
       " (269.5674047783832, 14886),\n",
       " (269.5027559152513, 367),\n",
       " (269.5027559152513, 1015),\n",
       " (269.4390978527226, 12958),\n",
       " (269.26239371017436, 1187),\n",
       " (269.2040288289427, 11972),\n",
       " (268.8765069041183, 24321),\n",
       " (268.6654379963438, 491),\n",
       " (268.53479393089947, 24950),\n",
       " (268.2930260503199, 16894),\n",
       " (268.0470666857145, 1180),\n",
       " (267.66601168684065, 12529),\n",
       " (267.6388085431536, 28827),\n",
       " (266.51435949647566, 36407),\n",
       " (265.337683857535, 7125),\n",
       " (265.1506420543301, 29933),\n",
       " (265.1506420543301, 30402),\n",
       " (265.07354464306263, 28710),\n",
       " (265.0313803567551, 27532),\n",
       " (264.9831325536652, 13267),\n",
       " (264.92545667035665, 14365),\n",
       " (264.82644702811376, 19299),\n",
       " (264.80801328035886, 28225),\n",
       " (264.3924347807697, 15933),\n",
       " (264.3924347807697, 16496),\n",
       " (264.3888767272001, 27220),\n",
       " (264.319325843564, 14184),\n",
       " (264.04716211915365, 25187),\n",
       " (263.83052630577004, 14419),\n",
       " (263.82546640522196, 29929),\n",
       " (263.82546640522196, 30398),\n",
       " (263.65300773183117, 30011),\n",
       " (263.65300773183117, 31692),\n",
       " (261.51548873333377, 743),\n",
       " (261.4431197124359, 37588),\n",
       " (261.25276248493174, 9573),\n",
       " (260.97499906405574, 15158),\n",
       " (260.8296950228396, 28487),\n",
       " (260.5902550792671, 2773),\n",
       " (260.5902550792671, 5195),\n",
       " (260.5902550792671, 33521),\n",
       " (260.30226601238974, 30020),\n",
       " (260.30226601238974, 32001),\n",
       " (260.30226601238974, 32068),\n",
       " (260.30226601238974, 32147),\n",
       " (260.24831624221406, 6713),\n",
       " (259.9910794576281, 17167),\n",
       " (259.8813938673702, 15662),\n",
       " (259.8813938673702, 16201),\n",
       " (259.7050890723185, 29616),\n",
       " (259.5469171165314, 1468),\n",
       " (259.5469171165314, 4416),\n",
       " (258.81762034747953, 14359),\n",
       " (258.62626209045993, 26539),\n",
       " (258.1407014698925, 38834),\n",
       " (257.9688901097106, 12204),\n",
       " (257.89498526148964, 3094),\n",
       " (257.8749446273141, 14641),\n",
       " (257.24889424676076, 19349),\n",
       " (256.6596193385194, 7932),\n",
       " (256.50302724030917, 16081),\n",
       " (256.50302724030917, 16642),\n",
       " (256.27053472516127, 22374),\n",
       " (256.2632388175698, 24366),\n",
       " (256.2632388175698, 31923),\n",
       " (256.04614980131737, 25828),\n",
       " (255.97890302259475, 38207),\n",
       " (255.7914581969235, 35980),\n",
       " (255.01352912181756, 28935),\n",
       " (254.97354555828497, 28157),\n",
       " (254.43498402560363, 19769),\n",
       " (253.61615085188532, 27964),\n",
       " (253.29941574746044, 27941),\n",
       " (253.24565821682336, 34647),\n",
       " (253.17303440760588, 13318),\n",
       " (252.90134789637523, 34170),\n",
       " (252.75494100147625, 37624),\n",
       " (252.2031519808079, 38594),\n",
       " (251.91979540453758, 7952),\n",
       " (251.70354210125515, 24092),\n",
       " (251.70354210125515, 31623),\n",
       " (251.1947033786564, 14728),\n",
       " (250.9388692099601, 13923),\n",
       " (250.56857988919364, 33569),\n",
       " (250.52312746626558, 235),\n",
       " (250.52312746626558, 942),\n",
       " (250.52312746626558, 1287),\n",
       " (250.52312746626558, 1974),\n",
       " (250.52312746626558, 2036),\n",
       " (250.52312746626558, 4283),\n",
       " (250.52312746626558, 4331),\n",
       " (250.52312746626558, 4377),\n",
       " (250.52312746626558, 4448),\n",
       " (250.52312746626558, 33188),\n",
       " (250.52312746626558, 33236),\n",
       " (250.52312746626558, 33276),\n",
       " (250.52312746626558, 33333),\n",
       " (250.32179642491974, 27494),\n",
       " (250.28857396281091, 17548),\n",
       " (250.11496235689265, 25847),\n",
       " (250.02111674458138, 14525),\n",
       " (249.8077547243447, 8632),\n",
       " (249.57699596491875, 7349),\n",
       " (249.52270474197576, 23711),\n",
       " (249.52270474197576, 31235),\n",
       " (249.4878792630625, 5010),\n",
       " (249.47894644306507, 22693),\n",
       " (248.84425387898227, 13095),\n",
       " (248.7048765696236, 8041),\n",
       " (248.58167337597115, 17864),\n",
       " (248.00244268203096, 19145),\n",
       " (247.88991821566015, 19984),\n",
       " (247.08078701942577, 23803),\n",
       " (247.08078701942577, 31327),\n",
       " (246.88106460653944, 19492),\n",
       " (246.75246954750037, 33566),\n",
       " (246.32536850488395, 25300),\n",
       " (246.2858316495549, 26157),\n",
       " (246.2194028442609, 26313),\n",
       " (246.14355673405953, 31872),\n",
       " (246.01508222370467, 18764),\n",
       " (245.48471820613486, 17434),\n",
       " (245.25268053490436, 16867),\n",
       " (244.86074433953036, 7732),\n",
       " (244.84710239334527, 38428),\n",
       " (244.78072415635688, 19267),\n",
       " (244.5358444149606, 27990),\n",
       " (244.44096400239505, 14923),\n",
       " (244.1096890362096, 14644),\n",
       " (244.099383197492, 1809),\n",
       " (244.099383197492, 2130),\n",
       " (244.03981474420289, 14251),\n",
       " (243.8386175337364, 24057),\n",
       " (243.8386175337364, 31584),\n",
       " (243.77247742150212, 36861),\n",
       " (243.71576296153944, 27798),\n",
       " (243.58551724781864, 12094),\n",
       " (243.3003441998153, 738),\n",
       " (243.3003441998153, 1228),\n",
       " (243.0479571391479, 14999),\n",
       " (242.89644007734023, 24275),\n",
       " (242.89644007734023, 31826),\n",
       " (242.75902658657287, 28116),\n",
       " (242.54878160648514, 23265),\n",
       " (242.54878160648514, 30787),\n",
       " (242.54388880479382, 4990),\n",
       " (242.44179818942212, 23669),\n",
       " (242.44179818942212, 31193),\n",
       " (242.3085596599849, 37546),\n",
       " (242.0690418445738, 22949),\n",
       " (242.0690418445738, 30453),\n",
       " (241.8076918714214, 15908),\n",
       " (241.8076918714214, 16471),\n",
       " (241.78711166181893, 19165),\n",
       " (240.8042173689464, 16027),\n",
       " (240.8042173689464, 16588),\n",
       " (240.3716724337428, 10688),\n",
       " (240.2969952467247, 23788),\n",
       " (240.2969952467247, 31312),\n",
       " (239.79929921952134, 28896),\n",
       " (239.30808844820422, 3250),\n",
       " (239.30446775391465, 28108),\n",
       " (239.14096792700002, 28040),\n",
       " (238.96364703646395, 8930),\n",
       " (238.57452272222145, 16032),\n",
       " (238.57452272222145, 16593),\n",
       " (238.34866983810207, 6476),\n",
       " (237.9920159112662, 38146),\n",
       " (237.8558186471928, 6515),\n",
       " (237.4642060615006, 27118),\n",
       " (236.86342943791533, 26779),\n",
       " (236.42262847459642, 20334),\n",
       " (236.42262847459642, 20757),\n",
       " (236.42262847459642, 21226),\n",
       " (236.42262847459642, 21695),\n",
       " (236.42262847459642, 22164),\n",
       " (236.15531953517348, 37564),\n",
       " (235.93545441195602, 25593),\n",
       " (235.7544003354269, 4603),\n",
       " (235.7544003354269, 4852),\n",
       " (235.67252301907865, 10095),\n",
       " (235.529894166335, 13026),\n",
       " (235.42703361116583, 36327),\n",
       " (235.19624602084514, 1439),\n",
       " (235.0607774859527, 36133),\n",
       " (235.03453307003656, 37873),\n",
       " (234.80342264562205, 13995),\n",
       " (234.06570034561446, 27714),\n",
       " (234.02634888318426, 39308),\n",
       " (234.00548034168605, 9369),\n",
       " (233.91279533214401, 31997),\n",
       " (233.77637924950977, 6480),\n",
       " (233.7321806576074, 5636),\n",
       " (233.7321806576074, 30243),\n",
       " (233.61591314716497, 36416),\n",
       " (233.28693503171962, 11150),\n",
       " (233.15417984500527, 33832),\n",
       " (233.0584478325618, 34865),\n",
       " (233.03676233519218, 23990),\n",
       " (233.03676233519218, 31517),\n",
       " (232.78399236447876, 13614),\n",
       " (232.70547727728263, 20107),\n",
       " (232.70547727728263, 37343),\n",
       " (232.68682946413173, 35710),\n",
       " (232.58965617557988, 38733),\n",
       " (232.58965617557988, 39348),\n",
       " (232.49601909898047, 27904),\n",
       " (232.30466855171835, 33086),\n",
       " (231.47293725558848, 23019),\n",
       " (231.47293725558848, 30523),\n",
       " (231.1320799437235, 28233),\n",
       " (231.1148072433425, 15565),\n",
       " (231.1148072433425, 16108),\n",
       " (231.09828238590853, 10297),\n",
       " (230.76226991578005, 14656),\n",
       " (230.51764634148276, 3511),\n",
       " (230.23362820020702, 11609),\n",
       " (230.11527109937742, 783),\n",
       " (229.97785578714684, 28922),\n",
       " (229.83852272987133, 29214),\n",
       " (229.71102847944712, 17624),\n",
       " (229.6631544266129, 23875),\n",
       " (229.6631544266129, 31402),\n",
       " (229.58661435206886, 24567),\n",
       " (228.9265299170802, 24775),\n",
       " (228.59732223766332, 25592),\n",
       " (228.34142796701053, 17490),\n",
       " (227.5608814193547, 48),\n",
       " (227.5608814193547, 884),\n",
       " (227.28687382256612, 17604),\n",
       " (227.21805070660776, 38067),\n",
       " (227.21646932995645, 16777),\n",
       " (226.79384754097555, 12973),\n",
       " (226.77724084368674, 23673),\n",
       " (226.77724084368674, 31197),\n",
       " (226.65146726924286, 5345),\n",
       " (226.65146726924286, 5837),\n",
       " (226.5169134173193, 10713),\n",
       " (226.16479715856258, 7005),\n",
       " (226.12905648359447, 2438),\n",
       " (226.10209714330267, 3364),\n",
       " (226.0750760009396, 36391),\n",
       " (225.82432896480896, 3157),\n",
       " (225.75884880294325, 7443),\n",
       " (225.65777407365385, 28415),\n",
       " (225.40102697029943, 26476),\n",
       " (225.32387638342334, 28424),\n",
       " (225.27849565813085, 17315),\n",
       " (224.64002515553148, 13386),\n",
       " (224.50946661020862, 7855),\n",
       " (224.43104948831024, 7062),\n",
       " (224.37019219604554, 20333),\n",
       " (224.37019219604554, 20756),\n",
       " (224.37019219604554, 21225),\n",
       " (224.37019219604554, 21694),\n",
       " (224.37019219604554, 22163),\n",
       " (224.29218001681147, 9386),\n",
       " (224.19543881702702, 5205),\n",
       " (224.19543881702702, 5696),\n",
       " (224.1531016220979, 39490),\n",
       " (224.1531016220979, 39604),\n",
       " (223.66854381852318, 35889),\n",
       " (223.6139306238474, 26218),\n",
       " (223.55979315080913, 401),\n",
       " (223.55979315080913, 1025),\n",
       " (223.48348517200793, 6024),\n",
       " (223.43222578652785, 7434),\n",
       " (223.39882942645636, 22692),\n",
       " (223.2927190043847, 8058),\n",
       " (222.73576064576628, 17186),\n",
       " (222.5601729255286, 22278),\n",
       " (222.2397174834623, 29881),\n",
       " (222.2397174834623, 30350),\n",
       " (221.92828737181844, 38310),\n",
       " (221.69740119684138, 8463),\n",
       " (221.2189225378679, 26718),\n",
       " (221.0255263975123, 25978),\n",
       " (221.02297059586272, 27536),\n",
       " (220.9024466630508, 16402),\n",
       " (220.77948189082235, 25829),\n",
       " (220.76918890041998, 11748),\n",
       " (220.76849009867874, 8648),\n",
       " (220.76286300205538, 35426),\n",
       " (220.66254166135332, 13074),\n",
       " (220.38127075636294, 34649),\n",
       " (220.08814665972022, 36621),\n",
       " (220.0181848112843, 8444),\n",
       " (219.93266001838492, 11288),\n",
       " (219.90240666578757, 11597),\n",
       " (219.71962816404994, 12301),\n",
       " (219.7048824413796, 27927),\n",
       " (219.48821149709693, 11078),\n",
       " (219.45420411619125, 24713),\n",
       " (219.37692186393542, 924),\n",
       " (219.04738016374176, 26732),\n",
       " (218.81744275976962, 12532),\n",
       " (218.67036128387554, 29867),\n",
       " (218.67036128387554, 30336),\n",
       " (218.31805776963301, 9249),\n",
       " (218.18685514692334, 19491),\n",
       " (217.92462269449607, 12609),\n",
       " (217.80192209902452, 325),\n",
       " (217.80192209902452, 1001),\n",
       " (217.5784103665501, 14978),\n",
       " (217.5746732729458, 7834),\n",
       " (217.45158561278367, 8820),\n",
       " (217.17452746850904, 36058),\n",
       " (216.97688847268, 9617),\n",
       " (216.6692022606294, 20329),\n",
       " (216.6692022606294, 20752),\n",
       " (216.6692022606294, 21221),\n",
       " (216.6692022606294, 21690),\n",
       " (216.6692022606294, 22159),\n",
       " (216.6134406455676, 19900),\n",
       " (216.4337051101029, 3849),\n",
       " (216.4337051101029, 18550),\n",
       " (216.36127890003263, 25853),\n",
       " (216.316491879581, 20312),\n",
       " (216.316491879581, 20735),\n",
       " (216.316491879581, 21204),\n",
       " (216.316491879581, 21673),\n",
       " (216.316491879581, 22142),\n",
       " (216.28680725666345, 38092),\n",
       " (216.2308542202809, 757),\n",
       " (216.2308542202809, 1231),\n",
       " (216.2308542202809, 2309),\n",
       " (216.2308542202809, 5147),\n",
       " (216.2308542202809, 33417),\n",
       " (216.09385733760428, 25697),\n",
       " (216.02162832088652, 15702),\n",
       " (216.0054326162208, 2577),\n",
       " (215.98118671932025, 37452),\n",
       " (215.9639811109082, 5955),\n",
       " (215.94522120960755, 28391),\n",
       " (215.7648238010588, 28771),\n",
       " (215.64940361719346, 12207),\n",
       " (215.63489599627792, 7947),\n",
       " (215.45480671142286, 386),\n",
       " (215.18733317073202, 39244),\n",
       " (215.17718007968506, 38572),\n",
       " (215.1137603269308, 23472),\n",
       " (215.1137603269308, 30994),\n",
       " (215.08077299175784, 22691),\n",
       " (214.99764405057067, 24),\n",
       " (214.99764405057067, 860),\n",
       " (214.78930272877915, 11121),\n",
       " (214.77847505389946, 14860),\n",
       " (214.51882633549394, 13591),\n",
       " (214.47817560227122, 13889),\n",
       " (214.17081165144918, 26633),\n",
       " (214.14565327615128, 39037),\n",
       " (213.76305097306613, 10314),\n",
       " (213.74024009951972, 27877),\n",
       " (213.6430080893624, 7854),\n",
       " (213.59858957288088, 12147),\n",
       " (213.51587601055508, 9140),\n",
       " (213.33308721116555, 15827),\n",
       " (213.33308721116555, 16380),\n",
       " (213.32297703172662, 23755),\n",
       " (213.32297703172662, 31279),\n",
       " (213.02346066915197, 12586),\n",
       " (212.8129749641812, 7446),\n",
       " (212.51425873686094, 39303),\n",
       " (212.34581772395177, 24839),\n",
       " (212.34581772395177, 32481),\n",
       " (212.34581772395177, 32779),\n",
       " (212.0906665357179, 35871),\n",
       " (212.0322554076265, 13213),\n",
       " (211.97842644318007, 1267),\n",
       " (211.97829790698597, 27928),\n",
       " (211.77479580114596, 23959),\n",
       " (211.77479580114596, 31486),\n",
       " (211.39174382333294, 20139),\n",
       " (211.39174382333294, 20556),\n",
       " (211.39174382333294, 21017),\n",
       " (211.39174382333294, 21486),\n",
       " (211.39174382333294, 21955),\n",
       " (211.32650017747073, 19155),\n",
       " (211.28610382700572, 3248),\n",
       " (210.93535767966387, 1281),\n",
       " (210.93535767966387, 4325),\n",
       " (210.93535767966387, 33230),\n",
       " (210.92818459437694, 265),\n",
       " (210.92818459437694, 947),\n",
       " (210.91253095376305, 24375),\n",
       " (210.91253095376305, 31932),\n",
       " (210.7365871064394, 17542),\n",
       " (210.6747991410375, 28175),\n",
       " (210.67437297431752, 12016),\n",
       " (210.6628415795276, 17567),\n",
       " (210.65890935481002, 31614),\n",
       " (210.327300692079, 11216),\n",
       " (210.26917592837708, 28112),\n",
       " (210.0242350572371, 27962),\n",
       " (209.6505294389499, 19173),\n",
       " (209.62077668233542, 36088),\n",
       " (209.60689283005195, 17035),\n",
       " (209.3467485132278, 4104),\n",
       " (209.16360318963416, 28037),\n",
       " (209.0757638804498, 2396),\n",
       " (209.0757638804498, 33437),\n",
       " (208.9763233702979, 39231),\n",
       " (208.73562571645016, 20222),\n",
       " (208.73562571645016, 20645),\n",
       " (208.73562571645016, 21114),\n",
       " (208.73562571645016, 21583),\n",
       " (208.73562571645016, 22052),\n",
       " (208.45415522783878, 16239),\n",
       " (208.4411131652887, 22379),\n",
       " (208.3525192271336, 8220),\n",
       " (208.33785986615112, 38636),\n",
       " (208.3368850001716, 12251),\n",
       " (208.23123347030196, 28264),\n",
       " (208.201432292175, 24215),\n",
       " (208.201432292175, 31754),\n",
       " (208.09727833687793, 17768),\n",
       " (207.48677427117946, 13376),\n",
       " (207.14080673959688, 35086),\n",
       " (207.04485477149137, 29631),\n",
       " (206.9955235956295, 37487),\n",
       " (206.9906761861348, 4176),\n",
       " (206.79974540433614, 8318),\n",
       " (206.66819609393133, 23681),\n",
       " (206.66819609393133, 31205),\n",
       " (206.50838926323922, 14007),\n",
       " (206.41668688657228, 28428),\n",
       " (205.86317796469666, 12663),\n",
       " (205.79437068600964, 14091),\n",
       " (205.76952555280877, 125),\n",
       " (205.6538570108387, 319),\n",
       " (205.6538570108387, 995),\n",
       " (205.58433196443366, 28206),\n",
       " (205.35985947510926, 749),\n",
       " (205.2942964828544, 36749),\n",
       " (205.28619250125485, 15887),\n",
       " (205.28619250125485, 16450),\n",
       " (204.7939795149141, 20228),\n",
       " (204.7939795149141, 20651),\n",
       " (204.7939795149141, 21120),\n",
       " (204.7939795149141, 21589),\n",
       " (204.7939795149141, 22058),\n",
       " (204.78220764145954, 1911),\n",
       " (204.78220764145954, 2169),\n",
       " (204.5338418315514, 6517),\n",
       " (204.5085612052353, 3249),\n",
       " (204.33687443105737, 36177),\n",
       " (204.2196022195858, 20244),\n",
       " (204.2196022195858, 20667),\n",
       " (204.2196022195858, 21136),\n",
       " (204.2196022195858, 21605),\n",
       " (204.2196022195858, 22074),\n",
       " (204.1731071783579, 8753),\n",
       " (204.1731071783579, 12219),\n",
       " (203.74955190745095, 29215),\n",
       " (203.69108514714753, 15245),\n",
       " (203.66379200053052, 16688),\n",
       " (203.6483876714774, 19902),\n",
       " (203.23172340678866, 136),\n",
       " (203.13027348491596, 24120),\n",
       " (203.13027348491596, 31651),\n",
       " (203.11964584165253, 19207),\n",
       " (203.10104590695119, 446),\n",
       " (202.33265723625664, 35762),\n",
       " (202.3202320569544, 18422),\n",
       " (202.10355442216678, 13826),\n",
       " (202.0498914982163, 16071),\n",
       " (202.0498914982163, 16632),\n",
       " (202.04640788427787, 37969),\n",
       " (201.94643964635907, 20014),\n",
       " (201.68016015176545, 37830),\n",
       " (201.51614256569883, 34863),\n",
       " (201.47568337776465, 33032),\n",
       " (201.22512138367165, 6571),\n",
       " (201.06526374415262, 28661),\n",
       " (201.00577992002945, 22440),\n",
       " (200.95603945590847, 14379),\n",
       " (200.86110200505937, 22974),\n",
       " (200.86110200505937, 30478),\n",
       " (200.60646531236125, 17729),\n",
       " (200.43365348194493, 35209),\n",
       " (200.2817890092556, 23769),\n",
       " (200.2817890092556, 31293),\n",
       " (200.15303729947482, 3703),\n",
       " (200.15303729947482, 4722),\n",
       " (199.9261080094875, 25334),\n",
       " (199.8897652924643, 24861),\n",
       " (199.8897652924643, 32503),\n",
       " (199.8897652924643, 32801),\n",
       " (199.80273254467465, 13153),\n",
       " (199.46395463700173, 19074),\n",
       " (199.4605812750233, 24105),\n",
       " (199.4605812750233, 31636),\n",
       " (199.1535391129437, 38740),\n",
       " (199.1535391129437, 39355),\n",
       " (198.7524876363459, 18055),\n",
       " (198.58937981020426, 13010),\n",
       " (198.45488307325286, 24539),\n",
       " (198.43858012271812, 913),\n",
       " (198.36280440707924, 14113),\n",
       " (197.93827682966366, 38016),\n",
       " (197.88457773337723, 20300),\n",
       " (197.88457773337723, 20723),\n",
       " (197.88457773337723, 21192),\n",
       " (197.88457773337723, 21661),\n",
       " (197.88457773337723, 22130),\n",
       " (197.74341640039347, 6913),\n",
       " (197.7356225103722, 20041),\n",
       " (197.60836529330118, 1381),\n",
       " (197.59723735542502, 17483),\n",
       " (197.1968846776581, 35394),\n",
       " (197.1609854507551, 34634),\n",
       " (197.12692067207536, 16356),\n",
       " (197.03460025088862, 24022),\n",
       " (197.03460025088862, 31549),\n",
       " (196.93483853182988, 20349),\n",
       " (196.93483853182988, 20772),\n",
       " (196.93483853182988, 21241),\n",
       " (196.93483853182988, 21710),\n",
       " (196.93483853182988, 22179),\n",
       " (196.66808044171194, 28176),\n",
       " (196.6378071710933, 28427),\n",
       " (196.62536618724698, 34358),\n",
       " (196.62204635929083, 18214),\n",
       " (196.5142154521891, 27988),\n",
       " (196.40625702776015, 12217),\n",
       " (196.22018792413292, 23029),\n",
       " (196.22018792413292, 30533),\n",
       " ...]"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Never say to a famous person that this possible endorsment would bring 'er to the spot light.\n",
      "[1196.38] KIM: And you, too, okay?\n",
      "\t(Keeping Up With the Kardashians - The Price of Fame)\n",
      "\n",
      "[904.11] KRIS: That last thing that I want to do is have Bruce get his feelings hurt.\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "\n",
      "[904.11] KRIS: That last thing that I want to do is have Bruce get his feelings hurt.\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "\n",
      "[831.56] KIM: That it was the best way to kick off our meeting.\n",
      "\t(Keeping Up With the Kardashians - Body Blows)\n",
      "\n",
      "[813.60] BRUCE: So I said to her father that I'd always take care of her.\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "\n",
      "[813.60] BRUCE: So I said to her father that I'd always take care of her.\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "\n",
      "[796.69] KOURTNEY: I really cannot believe Scott.\n",
      "\t(Keeping Up With the Kardashians - Blame It on the Alcohol)\n",
      "\n",
      "[705.12] KRIS: Tonight, I'm taking the BG5 girls-- the band I manage-- out to dinner.\n",
      "\t(Keeping Up With the Kardashians - Kris ``The Cougar'' Jenner)\n",
      "\n",
      "[689.48] KRIS: Khloe, I'm gonna do the best I can to keep my mouth shut, but I promise you that if Bruce finds out, this is gonna blow up in our face.\n",
      "\t(Keeping Up With the Kardashians - The Wedding)\n",
      "\n",
      "[689.48] KRIS: Khloe, I'm gonna do the best I can to keep my mouth shut, but I promise you that if Bruce finds out, this is gonna blow up in our face.\n",
      "\t(The Wedding: Keeping Up With the Kardashians)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(queries[1])\n",
    "\n",
    "for score, msg_id in score[:10]:\n",
    "    print(\"[{:.2f}] {}: {}\\n\\t({})\".format(\n",
    "        score,\n",
    "        flat_msgs[msg_id]['speaker'],\n",
    "        flat_msgs[msg_id]['text'],\n",
    "        flat_msgs[msg_id]['episode_title'])) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
